---
title: "Lecture 21 - Ensemble models - part III"
author: "Material written by Ryan Tibishirani and Presented by Renato Rodrigues Silva"
institute: "Federal University of Goias."
date: "(updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false




---
class: middle

##Review: Classification trees


<img src="Lecture_21_files/figure-html/Fig1.png" width="70%" align="center" />


---
class: middle

##Review: Classification trees


####CART algorithm:

The algorithm starts splitting on variable $j$ and split point $s$, defining the regions.

$$R_1 = \left\{ X \in \mathbb{R}^p: X_j \leq s \right\}, \phantom{111} R_2 = \left\{ X \in \mathbb{R}^p: X_j \leq s \right\}.$$



 Proportion of points of class $k$ is given by $\hat{p}_k(R_m) = \frac{1}{n_m}\sum_{x_i \in R_m}I(y_i = k).$ with $n_m$ the number of points in $R_m$. 
 
The most common class is $c_m = \mbox{argmax}_{k=1,\ldots,K}\hat{p}_k(R_m).$

The feature and split point are chosen recursively by minimizing the misclassification error $\mbox{argmin}_{j,s}\left([1 - \hat{p}_{c_1}(R_1) ] +[1 - \hat{p}_{c_2}(R_2) ]\right).$

---
class: middle

##Classification trees with observation weights

- Now suppose that we are going to add observation weights 
$w_i, i = 1, \ldots, n$.

- The weigthed proportion of points of class $k$ in region $R_m:$

$$\hat{p}_k(R_m) = \frac{\sum_{x_i \in R_m} w_i I(y_i = k)}{\sum_{x_i \in R_m} w_i},$$
As before, we let 

$$c_m = \mbox{argmax}_{k=1,\ldots, K} \hat{p}_k (R_m).$$
and hence $1 - \hat{p}_{c_m}(R_m)$ is the weighted misclassification error.


---
class: middle

##Boosting for Classification

- Boosting is an ensemble algorithm that transforms a set of weak learners into strong ones.

- Let's assume the training data $(x_i, y_i), i = 1, \ldots, n.$ and 
$y_i \in \left\{ -1, 1\right\}$ and $x_i \in \mathbb{R}^p.$

- Now let's assume that a predicted value of classification tree 
$\hat{f}^{\mbox{tree}}(x) \in \left\{ -1, 1\right\}$ for an input 
$x \in \mathbb{R}^p.$

In boositng we combine a **weighted sum** of $B$ different tree classfiers,

$$\hat{f}^{\mbox{boost}}(x) = \mbox{sign}(\sum_{b=1}^B \alpha_b \hat{f}^{\mbox{tree},b}(x))$$ 

 - Unlike in bagging and random forest, boosting use the entire training set.
 
 
---
class: middle

##Boosting


<img src="Lecture_21_files/figure-html/Fig2.png" width="70%" align="center" />


---
class: middle

##AdaBoost

- Given training data $(x_i, y_i), i = 1, \ldots, n$ the basic boosting method
is called **AdaBoost,** and can be described as:

- Initialize the weights by $w_i = \frac{1}{n}$ for each $i$

- For $b = 1, \ldots, B:$

1.  Fit a classification tree $\hat{f}^{tree, b}$ to the training data with
weights $w_1, \ldots, w_n.$

2.  Compute the weighted misclassification error 
$e_b = \frac{\sum_{i=1}^n w_i I(y_i \neq \hat{f}^{tree, b}(x_i))}{\sum_{i=1}^n w_i}.$

3.  Let $\alpha_b = \log\left\{ \frac{(1 - e_b)}{e_b} \right\}.$

4. Update the weights as  $w_i \leftarrow w_i \exp\left\{ \alpha_b I(y_i \neq \hat{f}^{tree, b}(x_i))\right\}$ for each $i$

Return $\hat{f}^{\mbox{boost}}(x) = \mbox{sign}(\sum_{b=1}^B \alpha_b \hat{f}^{\mbox{tree},b}(x))$


---
class: middle

##Conection between forward and boosting

###Review: Forward

- Given a continuous response  $y \in \mathbb{R}^p$ and predictors
$X_1, \ldots, X_p \in \mathbb{R}^n,$ we:

- Choose the predictor $X_j$ giving the  smallest squared error loss 
$\sum_{i=1}^n (y_i - \hat{\beta}_j X_{ij})^2$  

- Choose the predictor $X_k$ giving  the smallest additional loss 
$\sum_{i=1}^n (r_i - \hat{\beta}_k X_{ik})^2$  where $r$ is the residual defined by 
$r_i = y_i -  \hat{\beta}_j X_{ij}.$

- Repeat the last step.


---
class: middle

##Conection between forward and boosting

###Least Square Boosting

1 - Start with function $F_0(x) = 0$ and 
residual $r = y,$ $m = 0.$

2 - $m \leftarrow m + 1$

3 - Fit a CART regression tree to $r$ 
giving $g(x)$

4 - Set $f_m(x) \leftarrow \epsilon g(x)$

5 - Set $F_m(x) \leftarrow F_{m-1}(x) + f_m(x),$
$r \leftarrow r - f_m(x)$ and repeat step 2-5 many times.

- $g(x)$ typically shallow tree; e.g. constrained 
to have $k=3$ splits only (depth).

- $0 < \epsilon \leq 1$ is shrinkage parameter.

[Source:](https://www.youtube.com/watch?v=wPqtzj5VZus&t=1494s)

---
class: middle

##General Boosting Algorithm

The boosting paradigm can be extended 
to general loss functions - not only 
**squared error** or **exponential**.

1.  Initialize $F_0(x) = 0.$

2.  For $m = 1$ to $M:$

  a.  Compute
  $$(\beta_m, \gamma_m) = \mbox{argmin}_{\beta, \gamma}\sum_{i=1}^N L(y_i, F_{m-1}(x) + \beta_i b(x_i;\gamma))$$
  b.  Set $F_m(x) = F_{m-1}(x) + \epsilon\beta_m b(x;\gamma_m)$
  
Sometimes we define $\epsilon = 1.$

- Here $\epsilon$ is a shrinkage factor; in `gbm` package
in R, default is $\epsilon = 0.001.$

- Shrinkage slows the stagewise model-building  even more,
and tipically leads to better performance.

[Source:](https://www.youtube.com/watch?v=wPqtzj5VZus&t=1494s)

[Next:](http://uc-r.github.io/gbm_regression)
