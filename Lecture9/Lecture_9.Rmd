---
title: "Lecture 9 - Multiple Linear Regression part I"
author: "Material based on The Elements of Statistical Learning Data Mining, Inference, and Prediction (Hastie, Tibshirani and Friedman) and Presented by Renato Rodrigues Silva"
institute: "Federal University of Goias."
date: "(updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false


---
class: middle
## Introduction

- A linear regression model assumes that the regression function $E(Y |X)$ is linear in the inputs $X_1,... ,X_p$. 

- They are simple and often provide an adequate and interpretable description of how the inputs affect the output. 

- For prediction purposes they can sometimes outperform fancier
nonlinear models, especially in situations with small numbers of training
cases, low signal-to-noise ratio or sparse data. 


---
class: middle
##Linear Regression Models and Least Squares

Given a vector od inputs $X^{T} = \left(X_1, X_2, \ldots, X_p\right)$, we want to predict a real-valued output $Y$.

The linear regression model has the form:

$$E[Y|X] = f(X) = \beta_0 + \sum_{j=1}^p X_j \beta_j.$$

The linear model either assumes that the regression function $E(Y|X)$ is linear, or that the linear model is a reasonable approximation.


---
class: middle
##Linear Regression Models and Least Squares


Here $\beta_j^{'}$s are unknown parameters or coefficients, and the variables $X_j$ can come from different sources:

- Quantitative Inputs;

- Transformations of quantitative inputs, such as log, square-root or square;

- Basis expansions, such as $X_2 = X_1^2,$ $X_3 = X_1^3,$ leading to a polynomial representation;

- Numeric or "dummy" coding of the levels of qualitative inputs. For example, if $G$ is a five-level factor input, we might create $X_j, \phantom{1} j = 1, \ldots, 5.$ such that $X_j = I(G = j).$

- Together this group of $X_j$ represents the effect of $G$ by a set of level-dependent constants, since in $\sum_{j=1}^5 X_j\beta_j,$ one of the $X_j$s is one, and the others are zero. 

- Interactions between variables, for example, $X_3 = X_1 X_2.$

No matter the source of the $X_j$, the model is linear in the parameters.

---
class: middle
##Linear Regression Models and Least Squares

Given a set of training data $(x_1, y_1), \ldots, (x_N, y_N)$ we are going to estimate the parameters $\beta$.

The least square methods consists of estimate the regression coefficients minimizing the residual sum of squares

\begin{align}
RSS(\beta) =& \sum_{i=1}^{N}(y_i - f(x_i))^2 \\
           =& \sum_{i=1}^{N}(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j)^2.
\end{align}

From a statistical point of view, this criterion is reasonable if the training observations $(x_i, y_i)$ represent independent random draws from their population.

Even if the $x_i^{'}$s were not drawn randomly, the criterion is still valid if the  $y_i^{'}$s are conditionally independent given the inputs $x_i$.


---
class: middle
##Linear Regression Models and Least Squares

How to minimize the  residual sum of squares ?

--
$$RSS(\boldsymbol{\beta}) = (\mathbf{y} - \mathbf{X}\beta)^{'} (\mathbf{y} - \mathbf{X}\boldsymbol{\beta}).$$

--
$$\frac{\partial RSS}{\partial\boldsymbol{\beta}} = -2 \mathbf{X}^{'}(\mathbf{y} - \mathbf{X}\boldsymbol{\beta}).$$

--
$$\frac{\partial RSS}{\partial \boldsymbol{\beta}\boldsymbol{\beta}^{'}} = -2 \mathbf{X}^{'}\mathbf{X}.$$
--
Assuming that $\mathbf{X}$ has full column rank, and hence $\mathbf{X}^{'}\mathbf{X}$ is positive definite, we set the first derivative to zero

--
$$\mathbf{X}^{'}(\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) = \mathbf{0} \Rightarrow \mathbf{X}^{'}\mathbf{y} -\mathbf{X}^{'}\mathbf{X}\boldsymbol{\beta} = \mathbf{0} \Rightarrow \mathbf{X}^{'}\mathbf{X}\boldsymbol{\beta} = \mathbf{X}^{'}\mathbf{y}.$$
--
to obtain the unique solution

$$\hat{\boldsymbol{\beta}} = \left(\mathbf{X}^{'}\mathbf{X}\right)^{-1}\mathbf{X}^{'}\mathbf{y}.$$

---
##Properties of Estimator - Expectation

Assuming $\boldsymbol{\epsilon} \sim N(\mathbf{0}, \sigma^2\mathbf{I})$ and
$\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon},$ we have

--
$$E[\hat{\boldsymbol{\beta}}] = E\left[\left(\mathbf{X}^{'}\mathbf{X}\right)^{-1}\mathbf{X}^{'}\mathbf{y}\right]$$
--
$$E[\hat{\boldsymbol{\beta}}]=E\left[\left(\mathbf{X}^{'}\mathbf{X}\right)^{-1}\mathbf{X}^{'}[\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}]\right]$$
--
$$E[\hat{\boldsymbol{\beta}}]=E\left[\left(\mathbf{X}^{'}\mathbf{X}\right)^{-1}\mathbf{X}^{'}\mathbf{X}\boldsymbol{\beta}+  \left(\mathbf{X}^{'}\mathbf{X}\right)^{-1}\mathbf{X}^{'}\boldsymbol{\epsilon}\right]$$
--
$$E[\hat{\boldsymbol{\beta}}]=E\left[\left(\mathbf{X}^{'}\mathbf{X}\right)^{-1}\mathbf{X}^{'}\mathbf{X}\boldsymbol{\beta}\right]+   E\left[\left(\mathbf{X}^{'}\mathbf{X}\right)^{-1}\mathbf{X}^{'}\boldsymbol{\epsilon}\right]$$
--
$$E[\hat{\boldsymbol{\beta}}]=\left(\mathbf{X}^{'}\mathbf{X}\right)^{-1}\mathbf{X}^{'}\mathbf{X}E\left[\boldsymbol{\beta}\right]+   \left(\mathbf{X}^{'}\mathbf{X}\right)^{-1}\mathbf{X}^{'}E\left[\boldsymbol{\epsilon}\right]=\left(\mathbf{X}^{'}\mathbf{X}\right)^{-1}\mathbf{X}^{'}\mathbf{X}\boldsymbol{\beta}+   \left(\mathbf{X}^{'}\mathbf{X}\right)^{-1}\mathbf{X}^{'}E\left[\boldsymbol{\epsilon}\right] = \boldsymbol{\beta}.$$

---
##Properties of Estimator - Variance

--
$$Var[\hat{\boldsymbol{\beta}}] = Var\left[\left(\mathbf{X}^{'}\mathbf{X}\right)^{-1}\mathbf{X}^{'}\mathbf{y}\right]$$

--
$$Var[\hat{\boldsymbol{\beta}}]=Var\left[\left(\mathbf{X}^{'}\mathbf{X}\right)^{-1}\mathbf{X}^{'}[\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}]\right]$$


--
$$Var[\hat{\boldsymbol{\beta}}]=Var\left[\left(\mathbf{X}^{'}\mathbf{X}\right)^{-1}\mathbf{X}^{'}\mathbf{X}\boldsymbol{\beta}+\left(\mathbf{X}^{'}\mathbf{X}\right)^{-1}\mathbf{X}^{'}\boldsymbol{\epsilon}\right]$$
--
$$Var[\hat{\boldsymbol{\beta}}]=Var\left[\left(\mathbf{X}^{'}\mathbf{X}\right)^{-1}\mathbf{X}^{'}\boldsymbol{\epsilon} \right]$$


--
$$Var[\hat{\boldsymbol{\beta}}]= \left(\mathbf{X}^{'}\mathbf{X}\right)^{-1}\mathbf{X}^{'}Var\left[\boldsymbol{\epsilon}\right]\mathbf{X}\left(\mathbf{X}^{'}\mathbf{X}\right)^{-1}$$

--
$$Var[\hat{\boldsymbol{\beta}}]= \left(\mathbf{X}^{'}\mathbf{X}\right)^{-1}\mathbf{X}^{'}(\sigma^2\mathbf{I})\mathbf{X}\left(\mathbf{X}^{'}\mathbf{X}\right)^{-1}$$


--
$$Var[\hat{\boldsymbol{\beta}}]= \sigma^2\left(\mathbf{X}^{'}\mathbf{X}\right)^{-1}.$$


---
##Inference

$$\hat{\boldsymbol{\beta}} \sim N(\boldsymbol{\beta},\sigma^2\left(\mathbf{X}^{'}\mathbf{X}\right)^{-1} )$$
It is easy to show that

$$(N - p  - 1 )\hat{\sigma}^2 \sim \sigma^2 \chi^2_{(N-p-1)}.$$
where $\hat{\sigma}^2 = \frac{1}{N-p-1}\sum_{i=1}^N(y_i - \hat{y}_i)^2.$

---
##Hypothesis Testing


To test the hypothesis that a particular coefficient βj = 0, we form the
standardized coefficient or Z-score

$$z_j = \frac{\hat{\beta}_j}{\hat{\sigma}\sqrt{\nu_j}}.$$
where $\nu_j$ is the j-th diagonal element of $\left(\mathbf{X}^{'}\mathbf{X}\right)^{-1} )$.

Under the null hypothesis that $\beta_j = 0$, $z_j$ 
is distributed as $t_{N-p-1}$ (a t distribution with $N - p -1$ degrees of freedom)

Hence large absolute values of $z_j$ will lead to rejection of this null hypothesis.



---
class: middle


##F-test:  test for the significance of groups of coefficients simultaneously. 


$$F = \frac{\frac{RSS_0 - RSS_1}{p_1-p_0}}{\frac{RSS_1}{N-p_1-1}}.$$

where $RSS_1$ is the residual sum-of-squares for the least squares fit of the bigger model with p1+1 parameters, and $RSS_0$ the same for the nested smaller model with $p_0 +1$ parameters, having $p_1 −p_0$ parameters constrained to be zero.


---
class: middle
##F-test - Overall significance


The F-test for overall significance has the following two hypotheses:

- The null hypothesis states that the model with no independent variables fits the data as well as your model.
- The alternative hypothesis says that your model fits the data better than the intercept-only model.

$$F = \frac{ MSM}{MSE},$$

where MSM is the Mean fo Squares for Model and MSE is the Mean of Squares for Error.



---
class: middle
##Example Prostate - Cancer


```{r,warning = FALSE, message = FALSE}

library(tidyverse)
library(broom)

dat = read.table("http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/prostate.data")

nam = c("lcavol","lweight", "age", "lbph", "svi", "lcp","gleason", "pgg45")

dat.X = select(dat,one_of(nam))

round(cor(dat.X),3)


```


---
class: middle
##Example Prostate - Cancer


```{r,warning = FALSE, message = FALSE}


mod = lm(lpsa ~ ., data = dat)
tidy(mod)

```




---
class: middle
##Example Prostate - Cancer: Test for Overall significance


```{r,warning = FALSE, message = FALSE}

X =  as.matrix(dat.X)
mod = lm(lpsa ~ X, data = dat)
anova(mod)

```


---
class: middle
##Example Prostate - Cancer: Test for the significance of groups of coefficients simultaneously. 


```{r,warning = FALSE, message = FALSE}


mod = lm(lpsa ~ lcavol + lweight + age + lbph, data = dat)
anova(mod)

```


---
class: middle
##Example Prostate - Cancer: Test for the significance of groups of coefficients simultaneously. 


```{r,warning = FALSE, message = FALSE}


mod = lm(lpsa ~    lbph+ age +lweight +lcavol  , data = dat)
anova(mod)

```


---
class: middle
##Example Prostate - Cancer: Test for the significance of groups of coefficients simultaneously. 


```{r,warning = FALSE, message = FALSE}


mod = lm(lpsa ~ lcavol + lweight + age + lbph, data = dat)
mod2 =  lm(lpsa ~ ., data = dat)
anova(mod, mod2)

```

---
class: middle

## Subset Selection

There are two reasons why we are often not satisfied with the least squares
estimates 

- The first is prediction accuracy: the least squares estimates often have
low bias but large variance.

- Prediction accuracy can sometimes be improved by shrinking or setting some coefficients to zero. By doing so we sacrifice a little bit of bias to reduce the variance of the predicted
values, and hence may improve the overall prediction accuracy.

- The second reason is interpretation. With a large number of predictors, we often would like to determine a smaller subset that exhibit the strongest effects. 

---
class: middle


##Forward- and Backward-Stepwise Selection


- Rather than search through all possible subsets (which becomes infeasible
for $p$ much larger than 40), we can seek a good path through them. 

- Forward stepwise selection starts with the intercept, and then sequentially adds into the model the predictor that most improves the fit. 

- With many candidate predictors, this might seem like a lot of computation; however, clever updating algorithms can exploit the QR decomposition for the current fit to rapidly establish the next candidate. 


- Backward-stepwise selection starts with the full model, and sequentially
deletes the predictor that has the least impact on the fit. The candidate for dropping is the variable with the smallest Z-score.


---
class: middle

##Forward- and Backward-Stepwise Selection

- Some software packages implement hybrid stepwise-selection strategies
that consider both forward and backward moves at each step, and select
the “best” of the two. 

- For example in the R package the step function uses the AIC criterion for weighing the choices, which takes proper account of the number of parameters fit; at each step an add or drop will be performed that minimizes the AIC score.


- Other more traditional packages base the selection on F-statistics, adding
“significant” terms, and dropping “non-significant” terms.These are out of fashion, since they do not take proper account of the multiple testing
issues. 

- Finally, we note that often variables come in groups (such as the dummy
variables that code a multi-level categorical predictor). Smart stepwise procedures (such as step in R) will add or drop whole groups at a time, taking proper account of their degrees-of-freedom.

---
###Akaike information criterion

The Akaike information criterion (AIC) is an estimator of in-sample prediction error and thereby relative quality of statistical models for a given set of data.

In-sample prediction error is the expected error in predicting the resampled response to a training sample. 

Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. 

Thus, AIC provides a means for model selection. If out-of-sample prediction error is expected to differ from in-sample prediction error, cross-validation is a better estimate of model quality.

$$\displaystyle \mathrm {AIC} \,=\,2k-2\ln({\hat {L}})$$


[Source:](https://en.wikipedia.org/wiki/Akaike_information_criterion)
---
####Forward Regression


```{r,warning = FALSE, message = FALSE, eval=FALSE}

minimal.model = lm(lpsa ~ 1, data = dat)
forward.model = MASS::stepAIC(minimal.model,
                               scope = list(upper = ~ train+lcavol+ lweight+age+lbph+svi+lcp+gleason+pgg45,  lower = ~1),
                              direction="forward",trace = TRUE)  
  
tidy(forward.model) 

  

```



####Backward Regression


```{r,warning = FALSE, message = FALSE, eval=FALSE}

full.model = lm(lpsa ~ ., data = dat)
backward.model = MASS::stepAIC(full.model, 
                                scope = list(upper = ~ train+lcavol+ lweight+age+lbph+svi+lcp+gleason+pgg45,  lower = ~1),
                               direction="backward",trace = TRUE)  
  
tidy(backward.model) 

  

```

####Stepwise Regression


```{r,warning = FALSE, message = FALSE, eval=FALSE}

initial.model = lm(lpsa ~ 1, data = dat)
stepwise.model = MASS::stepAIC(initial.model, 
                                scope = list(upper = ~ train+lcavol+ lweight+age+lbph+svi+lcp+gleason+pgg45,  lower = ~1),
                               direction="both",trace = TRUE)  
tidy(stepwise.model)
  
  

```


---
class:middle
##Forward Regression

```{r,warning = FALSE, message = FALSE, echo=FALSE}

minimal.model = lm(lpsa ~ 1, data = dat)
forward.model = MASS::stepAIC(minimal.model,
                               scope = list(upper = ~ train+lcavol+ lweight+age+lbph+svi+lcp+gleason+pgg45,  lower = ~1),
                              direction="forward",trace = FALSE)  
  
tidy(forward.model) 

```

---
class: middle
##Backward Regression


```{r,warning = FALSE, message = FALSE, echo=FALSE}

full.model = lm(lpsa ~ ., data = dat)
backward.model = MASS::stepAIC(full.model, 
                                scope = list(upper = ~ train+lcavol+ lweight+age+lbph+svi+lcp+gleason+pgg45,  lower = ~1),
                               direction="backward",trace = FALSE)  
  
tidy(backward.model) 

```

---
class: middle
##Stepwise Regression


```{r,warning = FALSE, message = FALSE, echo=FALSE}

initial.model = lm(lpsa ~ 1, data = dat)
stepwise.model = MASS::stepAIC(initial.model, 
                                scope = list(upper = ~ train+lcavol+ lweight+age+lbph+svi+lcp+gleason+pgg45,  lower = ~1),
                               direction="both",trace = FALSE)  

tidy(stepwise.model)
  

```


