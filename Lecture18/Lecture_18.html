<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Lecture 18 - Ensemble models - part I</title>
    <meta charset="utf-8" />
    <meta name="author" content="Material based on Lecture Bagging - written by Ryan Tibshirani and Presented by Renato Rodrigues Silva" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Lecture 18 - Ensemble models - part I
### Material based on Lecture Bagging - written by Ryan Tibshirani and Presented by Renato Rodrigues Silva
### Federal University of Goias.
### (updated: 2020-10-26)

---

class: middle

##Learning Ensembles 

 **Ensemble Learning** consists of two steps:

1 - Construction of a **dictionary** 
`\(D = \left\{ T_1(X), T_2(X), \ldots, T_M(X) \right\}\)` of basis
elements (weak learners) `\(T_m(X).\)`

2-  Fitting a model `\(f(X) = \sum_{m \in D} \alpha_m T_m(X).\)`

Simple examples of ensembles

- Linear regression: The ensemble consists of the coordinate functions `\(T_m(X) = X_m.\)` The fitting is done by least square. (Unsual terminology)

- **Random Forests:** The ensemble consists of trees grown to 
bootstrapped versions of the data, 
with additional randomization at each split. The fitting simply averages.

- **Gradient Boosting:** The ensemble is grown in an adaptative
fashion, but then simply average.

[Source:](https://www.youtube.com/watch?v=wPqtzj5VZus&amp;t=1494s)

---
class: middle

##Review: Classification trees

- Our task is to predict the class label given 
a feature vector.

- Classification trees divide the feature space `\(\mathbb{R}^p\)` up into several rectangles, and then 
assign to each rectangle `\(R_j\)` a particular class `\(c_j.\)`

`$$\hat{f}^{\mbox{tree}} = \sum_{j=1}^m c_j 
I(x \in R_j) = c_j \phantom{11} \mbox{such that}
x \in R_j.$$`

- Given training data `\((x_i, y_i), i = 1, \ldots, n\)` with `\(y_i \in \left\{1, \ldots, K \right\}\)` being the class label and `\(x_j \in \mathbb{R}^p\)` the associated 
feature vector, the CART algorithm sucessively splits 
the features in a greedy fashion.

- In strategy is to grow a large tree and then prune
back using cross-validation. 

---
class: middle

##Review: Classification trees


- At the end, in each rectangle `\(R_j\)` the predicted class is simply the majority class:

`$$c_j = \mbox{arg max}_{k = 1,\ldots,K}\hat{p}_k (R_j),$$`
where `\(\hat{p}_k(R_j)\)` is the proportion of points of class `\(k\)` that fall into region `\(R_j\)`

`$$\hat{p}_k(R_j) = \frac{1}{n_j}\sum_{x \in R_j}I(y_i = k).$$`

- This gives us predicted class probabilities for each region.


---
class: middle

##Limitations of Trees

- One major problem of classification and regression trees are their high variance, mainly due to the hierarchical nature of the process.

  - Small change in data may result in a very different series of splits, making interpretations somewhat precautious.
  
  - The effect of an error in the top split is propagated down to all the splits below it.

---
class: middle

##Bootstrap 

- The **bootstrap** is a resampling method in statistics.

- The basic ideia underlying the bootstrap is that we can estimate the true probability distribution by the empirical distribution.

- A **bootstrap sample** of size `\(m\)` from the training data is 

`$$(x_i^{*}, y_i^{*}), \phantom{11} i = 1, \ldots, m.$$`
where each `\((x_i^{*}, y_i^{*})\)` are drawn from uniformily at random from `\((x_1^{*}, y_1^{*}), \ldots, (x_m^{*}, y_m^{*}),\)` with replacement.

- This corresponds exactly to `\(m\)` independent draws from `\(\hat{F}.\)`

- Note: **not all** of the training points are **represented** in a bootstrap sample, and some are represented more than once.

- About 36,8% of points are left out, for large `\(n\)`.



---
class: middle

##Bagging


- Bagging stands for ”Boostrap Aggregation” and is a variance reduction ensembling method.

- Given a training data `\((x_i, y_i), \phantom{1} i = 1, \ldots, n\)` bagging averages the predictions from classification trees over a collection of boostrap
samples.

- We draw `\(B\)` bootstrap samples and fit a classification tree for each one of them.

- At the end, to classify and input
`\(x \in \mathbb{R}^p,\)` we simply take the most 
commonly predicted class:

`$$\hat{f}^{\mbox{bag}}(x) = \mbox{argmax}_{k=1, \ldots, K} \sum_{b=1}^B I(\hat{f}^{\mbox{tree}}(x) = k).$$`

This is just choosing the class with the **most votes**. 


---
class: middle

##Bagging

###Voting: Two options

-  Simple strategy: grow fairly large trees on each sampled data set, with no pruning.

- More involved strategy: prune back each tree as we do with CART, but use the original training data `\((x_i, y_i), i = 1, \ldots, n\)` as the validation set, instead of performing cross-validation.


---
class: middle

##Bagging - Example

&lt;img src="Lecture_18_files/figure-html/Fig1.png" width="55%" align="center" /&gt;


---
class: middle

##Alternative form of bagging 

- Now given and input `\(x \in \mathbb{R}^p,\)`
instead of simply taking the prediction
`\(\hat{f}^{\mbox{tree}, b}(x)\)` from each tree

- We go further and look at its predicted class
probabilities `\(\hat{p}^{\mbox{tree},b}(x), k = 1, \ldots, K.\)` 

- We then define the bagging estimates of class
probabilities:

`$$\hat{p}_k^{\mbox{bag}}(x) = \frac{1}{B}
\sum_{b=1}^B \hat{p}_k^{\mbox{tree},b}(x), 
\phantom{11} k=1,\ldots,K.$$`

- The final bagged just chooses the class with the highest probability

`$$\hat{f}^{\mbox{bag}}(x) = \mbox{argmax}_{k=1, \ldots, K} \hat{p}_k^{\mbox{bag}}(x) .$$`
- This form of bagging is preferred if it is desired
to get estimates of the class probabilities.

- Also, it can sometimes help the overall prediction accuracy.

---
class: middle

##Why Does Bagging works? (“wisdom of the crowd”)

- The general idea behind bagging is referred to as the “wisdom of the crowd” effect and was popularized by Surowiecki (2005). 

- It  means that the aggregation of information in large diverse groups results in decisions that are often better than could have been made by any single member of the group. 

- The more diverse the group members are then the more diverse their perspectives and predictions will be, which often leads to better aggregated information.

[Source:](https://bradleyboehmke.github.io/HOML/bagging.html)

---
classe: middle

##Bagging - R code




```
## Bagged CART 
## 
## 102 samples
##   7 predictor
##   2 classes: '0', '1' 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 92, 93, 92, 92, 91, 92, ... 
## Resampling results:
## 
##   Accuracy   Kappa    
##   0.7650505  0.5174508
```


---
class: middle

##ROC curve

![](Lecture_18_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;



---
class: middle
##Example 

####Confusion Matrix

```
##          actual
## predicted  0  1
##         0 32  5
##         1  1  6
```

####Accuracy

```
##  Accuracy 
## 0.8636364
```

####Overall

```
##          Sensitivity          Specificity       Pos Pred Value 
##            0.5454545            0.9696970            0.8571429 
##       Neg Pred Value            Precision               Recall 
##            0.8648649            0.8571429            0.5454545 
##                   F1           Prevalence       Detection Rate 
##            0.6666667            0.2500000            0.1363636 
## Detection Prevalence    Balanced Accuracy 
##            0.1590909            0.7575758
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
