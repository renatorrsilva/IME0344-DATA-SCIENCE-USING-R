---
title: "Lecture 19 - Ensemble models - part II"
author: "Presented by Renato Rodrigues Silva"
institute: "Federal University of Goias."
date: "(updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false


---
class: middle

##Review: Classification trees

- Our task is to predict the class label given a feature vector.

- Classification trees divide the feature space $\mathbb{R}^p$ up into several rectangles, and then 
assign to each rectangle $R_j$ a particular class $c_j.$

$$\hat{f}^{\mbox{tree}} = \sum_{j=1}^m c_j 
I(x \in R_j) = c_j \phantom{11} \mbox{such that}
x \in R_j.$$

- Given training data $(x_i, y_i), i = 1, \ldots, n$ with $y_i \in \left\{1, \ldots, K \right\}$ being the class label and $x_j \in \mathbb{R}^p$ the associated 
feature vector, the CART algorithm sucessively splits 
the features in a greedy fashion.

- In strategy is to grow a large tree and then prune
back using cross-validation. 

---
class: middle

##Review: Classification trees


- At the end, in each rectangle $R_j$ the predicted class is simply the majority class:

$$c_j = \mbox{arg max}_{k = 1,\ldots,K}\hat{p}_k (R_j),$$
where $\hat{p}_k(R_j)$ is the proportion of points of class $k$ that fall into region $R_j$

$$\hat{p}_k(R_j) = \frac{1}{n_j}\sum_{x \in R_j}I(y_i = k).$$

- This gives us predicted class probabilities for each region.


---
class: middle

##Limitations of Trees

- One major problem of classification and regression trees are their high variance, mainly due to the hierarchical nature of the process.

  - Small change in data may result in a very different series of splits, making interpretations somewhat precautious.
  
  - The effect of an error in the top split is propagated down to all the splits below it.

---
classe: middle

##Random Forest - Overview

- The **random forest** algorithm **can be understood** as **bagging** with decision trees, **but instead** of **growing the decision trees** by basing the splitting criterion on the **complete feature set**, we use **random feature subsets**.


- To summarize, in random forests, we fit decision trees on different bootstrap samples.

- And in addition, for each decision tree, we select a random subset of features at each node to decide upon the optimal split; 

- While the size of the feature subset to consider at each node is a hyperparameter that we can tune, a “rule-of-thumb” suggestion is
to use $\mbox{NumFeatures} = \log2 m + 1.$

[Source:](https://github.com/rasbt/stat479-machine-learning-fs18/tree/master/07_ensembles)
---
class: middle

##Random forests using random input selection

- Earlier random decision forests by Tin Kam Ho used the “random subspace method,” where each tree got a random subset of features.

-  However, a few years later, Leo Breiman described the procedure of selecting different subsets of features for each node (while a tree was given the full set of features) 

— Leo Breiman’s formulation has become the “trademark” random forest algorithm that we typically refer to these days when we speak of “random forest."

- "The simplest random forest with random features is formed by selecting at random, at each node, a small group of input variables to split on".

[Source:](https://github.com/rasbt/stat479-machine-learning-fs18/tree/master/07_ensembles)
---
class: middle

##Random forests using linear combinations of inputs

- However, If there are only a few inputs, say $M$, taking $F$ an appreciable fraction of $M$ might lead an increase in strength but higher correlation.

- Another approach consists of defining more features by taking random linear combinations of a number of the input variables. 

- That is, a feature is generated by specifying $L$, the number of variables to be combined.


---
class: middle

##Generalization Error

- The reason why random forests may work better in practice than a regular bagging model, might be explained by the additional randomization that further diversifies the individual trees (i.e., decorrelates them).

- The upper bound of the generalization error is given as

$$\mbox{PE} \leq \frac{\bar{\rho}(1 - s^2)}{s^2}.$$
where $\bar{p}$ is the average correlation trees $s$ and measures the strength of trees as classifiers.

- As much the correlation goes down the lower error.

---
classe: middle

##Forest error rate

The forest error rate depends on two things:

1. The correlation between any two trees in the forest. Increasing the correlation increases the forest error rate.

2. The strength of each individual tree in the forest. A tree with a low error rate is a strong classifier. Increasing the strength of the individual trees decreases the forest error rate.

[Source:](https://www.listendata.com/2014/11/random-forest-with-r.html)


####Decorrelation 

 - The correlation is because we are growing trees to the 
 **same data**.
 
 - The correlation decreases as $m$ decreases, where $m$ is the
 number of variables sampled at each split.
 
[Source:](https://www.youtube.com/watch?v=wPqtzj5VZus&t=1494s) 

---
classe: middle

###Random Forest - Feature Selection Importance

- The measure based on which the (locally) optimal condition is chosen is called impurity. 

- For classification, it is typically either Gini impurity or information gain/entropy and for regression trees it is variance. 

- We can measure how each feature decrease the impurity of the split (the feature with highest decrease is selected for internal node). 

-  For each feature we can collect how on average it decreases the impurity. The average over all trees in the forest is the measure of the feature importance. 

[Source:](https://blog.datadive.net/selecting-good-features-part-iii-random-forests/.)

[Source 2:](https://mljar.com/blog/feature-importance-in-random-forest/)

---
classe: middle

###Random Forest - Feature Selection Importance

- Feature selection based on impurity reduction is biased towards preferring variables with more categories. 

- When the dataset has two (or more) correlated features, then from the point of view of the model, any of these correlated features can be used as the predictor, with no concrete preference of one over the others. 

- But once one of them is used, the importance of others is significantly reduced since effectively the impurity they can remove is already removed by the first feature. 

- As a consequence, they will have a lower reported importance. 

- This is not an issue when we want to use feature selection to reduce overfitting, since it makes sense to remove features that are mostly duplicated by other features. 

- But when interpreting the data, it can lead to the incorrect conclusion that one of the variables is a strong predictor while the others in the same group are unimportant,

- While actually they are very close in terms of their relationship with the response variable.

[Source 1:](https://blog.datadive.net/selecting-good-features-part-iii-random-forests/.)

---
classe: middle

### Feature Selection based on permutation test and Out-of-Bag Error 

1.  For each tree grown in a random forest, calculate number of votes for the correct class in out-of-bag data.

2.  Now perform random permutation of a predictor's values (let's say variable-k) in the oob data and then check the number of votes for correct class. 
  - By "random permutation of a predictor's values", it means changing the order of values (shuffling).

3.  Subtract the number of votes for the correct class in the variable-k-permuted data from the number of votes for the correct class in the original oob data.

4.  The average of this number over all trees in the forest is the raw importance score for variable k. The score is normalized by taking the standard deviation.

5.  Variables having large values for this score are ranked as more important. It is because if building a current model without original values of a variable gives worse prediction, it means the variable is important.

[Source:](https://www.listendata.com/2014/11/random-forest-with-r.html)


---
class: middle

##Random forest - Parameters Tunning


Two parameters are important in the random forest algorithm:

- Number of trees used in the forest (`ntree`) and

- Number of random variables used in each tree (`mtry`).


[Source:](https://www.listendata.com/2014/11/random-forest-with-r.html)

---
class: middle

##Random forest - Parameters Tunning

###Optimizing the `ntree`

- First set the mtry to the default value (sqrt of total number of all predictors) and search for the optimal `ntree` value.

- To find the number of trees that correspond to a stable classifier, we build random forest with different `ntree` values (100, 200, 300….,1,000). 

- We build 10 RF classifiers for each `ntree` value, record the OOB error rate and see the number of trees where the out of bag error rate stabilizes and reach minimum.


[Source:](https://www.listendata.com/2014/11/random-forest-with-r.html)

---
class: middle

##Random forest - Parameters Tunning

###Optimizing the `mtry`

- There are two ways to find the optimal mtry :

1.  Apply a similar procedure such that random forest is run 10 times. The optimal number of predictors selected for split is selected for which out of bag error rate stabilizes and reach minimum.

2.  Experiment with including the (square root of total number of all predictors), (half of this square root value), and (twice of the square root value). And check which mtry returns maximum Area under curve. Thus, for 1000 predictors the number of predictors to select for each node would be 16, 32, and 64 predictors.

[Source:](https://www.listendata.com/2014/11/random-forest-with-r.html)

---
classe: middle

####Random Forest - Optimizing the number of trees

```{r, warning=FALSE, message=FALSE, eval = FALSE}

library(tidyverse); library(rpart)
library(caret); library(pROC)
library(ROCit); library(randomForest)


#Dataset
dat = mutate(stagec, pgstat = factor(pgstat) )
dat$eet[is.na(dat$eet)] = median(dat$eet, na.rm=TRUE)
dat$g2[is.na(dat$g2)]= median(dat$g2, na.rm=TRUE)
dat$gleason[is.na(dat$gleason)]= median(dat$gleason, na.rm=TRUE)

set.seed(42)
dat_idx = sample(nrow(dat), round(0.7*nrow(dat)))
dat_trn = dat[dat_idx, ]
dat_tst = dat[-dat_idx, ]

X_trn = select(dat_trn,-pgstat)
y_trn = select(dat_trn, pgstat)

mtry_std = sqrt(ncol(X_trn))

ntree_100 =  randomForest(X_trn,y_trn, ntree=100, mtry=mtry_std )
ntree_200 =  randomForest(X_trn,y_trn, ntree=200, mtry=mtry_std )
ntree_300 =  randomForest(X_trn,y_trn, ntree=300, mtry=mtry_std )
ntree_400 =  randomForest(X_trn,y_trn, ntree=400, mtry=mtry_std )

```

---
classe: middle

####Random Forest - Optimizing the number of trees

```{r, warning=FALSE, message=FALSE, echo = FALSE}

library(tidyverse)
library(rpart)
library(caret)       
library(pROC)
library(ROCit)
library(randomForest)


#Dataset
dat = mutate(stagec, pgstat = factor(pgstat) )
dat$eet[is.na(dat$eet)] = median(dat$eet, na.rm=TRUE)
dat$g2[is.na(dat$g2)]= median(dat$g2, na.rm=TRUE)
dat$gleason[is.na(dat$gleason)]= median(dat$gleason, na.rm=TRUE)

set.seed(42)
dat_idx = sample(nrow(dat), round(0.7*nrow(dat)))
dat_trn = dat[dat_idx, ]
dat_tst = dat[-dat_idx, ]

X_trn = select(dat_trn,-pgstat)
y_trn = dat_trn$pgstat

mtry_std = sqrt(ncol(X_trn))

ntree_100 =  randomForest(X_trn,y_trn, ntree=100, mtry=mtry_std )
ntree_200 =  randomForest(X_trn,y_trn, ntree=200, mtry=mtry_std )
ntree_300 =  randomForest(X_trn,y_trn, ntree=300, mtry=mtry_std )
ntree_400 =  randomForest(X_trn,y_trn, ntree=400, mtry=mtry_std )
ntree_500 =  randomForest(X_trn,y_trn, ntree=500, mtry=mtry_std )
ntree_600 =  randomForest(X_trn,y_trn, ntree=600, mtry=mtry_std )


par(mfrow=c(3,2))
plot(1:100,ntree_100$err.rate[,1],type="l", ylab="oob",xlab="ntree",
     ylim=c(0.1,0.4))
plot(1:200,ntree_200$err.rate[,1],type="l", ylab="oob",xlab="ntree",
     ylim=c(0.1,0.4))
plot(1:300,ntree_300$err.rate[,1],type="l", ylab="oob",xlab="ntree",
     ylim=c(0.1,0.4))
plot(1:400,ntree_400$err.rate[,1],type="l", ylab="oob",xlab="ntree",
     ylim=c(0.1,0.4))
plot(1:500,ntree_500$err.rate[,1],type="l", ylab="oob",xlab="ntree",
     ylim=c(0.1,0.4))
plot(1:600,ntree_600$err.rate[,1],type="l", ylab="oob",xlab="ntree",
     ylim=c(0.1,0.4))

```


---
classe: middle

####Random Forest - Optimizing the mtry parameters

```{r, warning=FALSE, message=FALSE, eval = FALSE}

set.seed(1)
bestMtry <- tuneRF(X_trn,y_trn, stepFactor = 1.5, improve = 1e-5, ntree = 200,trace=FALSE,plot=FALSE)
bestMtry
```


---
class: middle
####Random Forest - Optimizing the mtry parameters

```{r, warning=FALSE, message=FALSE, echo = FALSE}

set.seed(1)
bestMtry <- tuneRF(X_trn,y_trn, stepFactor = 1.5, improve = 1e-5, ntree = 200,trace=FALSE,plot=FALSE)
bestMtry
```


---
class: middle
####Random Forest 

```{r, warning=FALSE, message=FALSE, eval = FALSE}

mod_rf = randomForest(X_trn,y_trn,ntree=200, mtry=3)
score = predict(mod_rf, newdata = dat_tst,type="prob")[,2]
obs = dat_tst$pgstat
ROCit_obj <- rocit(score=score,class=obs)
plot(ROCit_obj)
```


---
class: middle
####Random Forest 

```{r, warning=FALSE, message=FALSE, echo = FALSE}

mod_rf = randomForest(X_trn,y_trn,ntree=500, mtry=3)
score = predict(mod_rf, newdata = dat_tst,type="prob")[,2]
obs = dat_tst$pgstat
ROCit_obj <- rocit(score=score,class=obs)
plot(ROCit_obj)
```

---
class: middle
####Random Forest 

```{r, warning=FALSE, message=FALSE, eval = FALSE}

#Testing data
optimal_cutoff = ROCit_obj$Cutoff[which.max(ROCit_obj$TPR - ROCit_obj$FPR)]
model_bag_pred= ifelse(predict(mod_rf , newdata=dat_tst,type="prob")[,2] > optimal_cutoff, "1", "0")
train_tab = table(predicted = model_bag_pred, actual = as.character(dat_tst$pgstat))


```


```{r, warning=FALSE, message=FALSE, echo = FALSE}

#Testing data
optimal_cutoff = ROCit_obj$Cutoff[which.max(ROCit_obj$TPR - ROCit_obj$FPR)]
model_rf_pred= ifelse(predict(mod_rf, newdata=dat_tst,type="prob")[,2] > optimal_cutoff, "1", "0")
train_tab = table(predicted = model_rf_pred, actual = as.character(dat_tst$pgstat))
train_con_mat = confusionMatrix(train_tab, positive = "1")


```

---
class: middle
##Example 

####Confusion Matrix
```{r, warning = FALSE, message = FALSE, echo=FALSE}
train_con_mat$table
```

####Accuracy
```{r, warning = FALSE, message = FALSE, echo=FALSE}
train_con_mat$overall["Accuracy"]
```

####Overall
```{r, warning = FALSE, message = FALSE, echo=FALSE}
train_con_mat$byClass
```


[Source:](https://rpubs.com/phamdinhkhanh/389752)