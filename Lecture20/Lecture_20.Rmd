---
title: "Lecture 20 - Ensemble models - part III"
author: "Random Forest - Breiman 2001"
institute: "Federal University of Goias."
date: "(updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false




---
class: middle

##Abstract

- Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. 

- The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. 


- The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. 


- Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. 

- Internal estimates are also used to measure variable importance. 

- These ideas are also applicable to regression.



---
class: middle
##Prerequisites

###Chebyshev inequality

- If the variance is small, we have little randomness.

**Formal Definition:**

$$P(|X - \mu| \geq \epsilon ) \leq \frac{\sigma^2}{\epsilon^2}.$$

The probability that the distance from the mean is larger than or equal to certain number is, at most, the variance divided by the square of that number

[Source:](https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-ii-inference-limit-theorems/the-chebyshev-inequality/#:~:text=What%20the%20Chebyshev%20inequality%20says,we%20have%20the%20following%20inequality.)

---
class: middle
##Prerequisites - Classification Trees

<img src="Lecture_20_files/figure-html/Fig1.png" width="70%" align="center" />

[Source:](https://www.cs.cmu.edu/~bhiksha/courses/10-601/decisiontrees/)



---
class: middle
##Prerequisites

###The Strong Law of Large Numbers


**Definition:**
The strong law of large numbers states that the sample average converges almost surely to the expected value

$$\bar{X}_n \xrightarrow[]{a.s} \mu, \phantom{111} n \rightarrow \infty.$$
That is, 

$$Pr \left( \lim_{n \rightarrow \infty} \bar{X}_n = \mu\right) = 1.$$

What this means is that the probability that, as the number of trials $n$ goes to infinity, the average of the observations converges to the expected value, is equal to one.

[Source:](https://en.wikipedia.org/wiki/Law_of_large_numbers#Differences_between_the_weak_law_and_the_strong_law)

[Differences between the weak law and the strong law:](https://en.wikipedia.org/wiki/Law_of_large_numbers#Differences_between_the_weak_law_and_the_strong_law)


---
class: middle
##1. Introduction

**Definition 1.1** 

A random forest is a classifier consisting of a collection of tree-structured classifiers $\left\{h(\mathbf{x}, \Theta_k), k = 1, \ldots \right\}$ where the $\left\{ \Theta_k \right\}$ are independent identically distributed random vectors and each tree casts a unit vote for the most popular class at input $\mathbf{x}.$


####Question

- *What is meant by **random vectors** in Random forest ?*

- The parameters of the trees (these parameters include the structure of tree, which variables are split in which node, etc.)



---
class: middle
##2. Characterizing the accuracy of random forests

###2.1 Random forests converge

 - Given an ensemble of classifiers $h_1(\mathbf{x}), h_2(\mathbf{x}), \ldots, h_K(\mathbf{x})$, and with the training set drawn at random from the distribution of the random vector $Y$, $\mathbf{X},$ define the margin function as
 
$$mg(\mathbf{X},Y) = av_k I(h_k(\mathbf{X}) = Y) - \mbox{max}_{j \neq Y}  av_k I(h_k(\mathbf{X}) = j),$$

where $I(h_k(\mathbf{X}) = Y)$ is a indicator function such as:

\begin{align}
I(h_k(\mathbf{X}) = Y) = \left\{\begin{array}{cc} 
1, & \mbox{if} \phantom{11} h_k(\mathbf{X}) = Y\\
0, & \mbox{otherwise.} \end{array} \right.
\end{align}

- Here, $Y$ means the "right answer" and $j$ wrong answers.

- The margin measures the extent to which the average number of votes at $\mathbf{X}$, $Y$ for the right class exceeds the average vote for any other class.

- The larger the margin, the more confidence in the classification.

---
class: middle
##2. Characterizing the accuracy of random forests

####Generalization error

$$PE^{*} = P_{\mathbf{X},Y}(\mbox{mg}(\mathbf{X},Y) < 0).$$
where the subscripts $\mathbf{X},Y$ indicate that the probability is over $\mathbf{X}, Y$ space.


**Theorem 1.2** *As the number of trees increases, for almost surely all sequences* $\Theta_1, \ldots$ *the generalization error* $PE^{*}$ *converges to*

$$P_{\mathbf{X}, Y}(P_{\Theta}(h(\mathbf{X}, \Theta) = Y) -
\mbox{max}_{j \neq Y} P_{\Theta}(h(\mathbf{X}, \Theta) = j) < 0).$$

- This result explains why random forests do not overfit as more trees are added, but produce a limiting value of the generalization error.

---
class: middle
##Appendix I: Almost sure convergence

**Proof of theorem 1.2:** It suffices to show that there is a set of probability zero $C$ on the sequence space $\Theta_1, \Theta_2, \ldots$ such that outside of $C$, for all $\mathbf{x},$ 

$$\frac{1}{N} \sum_{n=1}^N I(h(\Theta_n, \mathbf{x})=j) \rightarrow P_{\Theta}(h(\Theta, \mathbf{x})=j).$$

- Here, $j$ is a general category (it is not only the wrong answers). 

---
class: middle
##Appendix I: Almost sure convergence

- For a fixed training set and fixed $\Theta,$ the set of all $\mathbf{x}$ such that $h(\Theta, \mathbf{x}) = j$ is a union of hyper-rectangles.

- For all $h(\Theta, \mathbf{x})$ there is only a finite number $K$ of such unions of hyper-rectangles, denoted by $S_1, \ldots, S_K.$


- Define $\varphi(\Theta_n) = k$ if $\left\{ \mathbf{x}: h(\Theta, \mathbf{x}) = j\right\} = S_k.$

- Let $N_k$ be the number of times that $\varphi(\Theta_n) = k$ in the first $N$ trials. Then

$$\frac{1}{N}\sum_{n=1}^N I(h(\Theta_n, \mathbf{x})=j) = \frac{1}{N}\sum_{k}N_k I(\mathbf{x} \in S_k).$$
- Here, there is a one to one relationship between $j$ and $k$ and $n$ is the index  of the $n$-th tree.


---
class: middle
##Appendix I: Almost sure convergence

By the Law of Large Numbers,

$$N_k = \frac{1}{N}\sum_{n=1}^N I(\varphi(\Theta_n) =k)$$
converges a.s to $P_{\Theta}(\varphi(\Theta) = k).$

- **Note:** $N_k$ it no longer the the number of times that $\varphi(\Theta_n) = k$ in the first $N$ trials.

- Taking unions of all the sets on which convergence does not occur form some value of $k$ gives a set $C$ of zero probability such that outside of $C$,

$$\frac{1}{N} \sum_{n=1}^N I(h(\Theta_n, \mathbf{x})=j) \rightarrow \sum_k P_{\Theta}(\varphi(\Theta) = k) I(\mathbf{x} \in S_k ).$$

The right hand side is $P_{\Theta}(h(\Theta, \mathbf{x})=j).$

---
class: middle
##2.2 Strength and correlation 

- An upper bound can be derived for the generalization error in terms of two parameters that are measures of how accurate the individual classifiers are and of the dependence between them.

###Definition 2.1 The margin function for a random forest is 

$$mr(\mathbf{X},Y) = P_{\Theta}(h(\mathbf{X},\Theta) = Y) - \mbox{max}_{j \neq Y} P_{\Theta}(h(\mathbf{X}, \Theta) = j)$$

and strength of the set of classifiers $\left\{h(\mathbf{X}, \Theta) \right\}$ is

$$s = E_{\mathbf{X},Y} \mbox{mr}(\mathbf{X},Y).$$

Assuming $s \leq 0$, Chebychevâ€™s inequality 

$$PE^{*} = P_{\mathbf{X},Y}(mr(\mathbf{X},Y) < 0) \leq P_{\mathbf{X},Y}\left( |mr(\mathbf{X},Y) - s| \geq s \right) \leq \frac{Var(mr(\mathbf{X},Y))}{s^2}.$$

- Why ???
---
class: middle
##2.2 Strength and correlation 

\begin{align}
&P_{\mathbf{X},Y}\left( |mr(\mathbf{X},Y) - s| \geq s \right) = \\
& P_{\mathbf{X},Y}\left( mr(\mathbf{X},Y) - s < -s\right) + P_{\mathbf{X},Y}\left(mr(\mathbf{X},Y) - s > s\right)  =\\
& P_{\mathbf{X},Y}(mr(\mathbf{X},Y) < 0) + P_{\mathbf{X},Y}\left(mr(\mathbf{X},Y) - s > s\right).
\end{align}

Therefore,

$$PE^{*} = P_{\mathbf{X},Y}(mr(\mathbf{X},Y) < 0) \leq P_{\mathbf{X},Y}\left( |mr(\mathbf{X},Y) - s| \geq s \right)$$
and consequently,

$$PE^{*} = P_{\mathbf{X},Y}(mr(\mathbf{X},Y) < 0) \leq P_{\mathbf{X},Y}\left( |mr(\mathbf{X},Y) - s| \geq s \right) \leq \frac{Var(mr(\mathbf{X},Y))}{s^2}.$$

---
class: middle
##2.2 Strength and correlation 


- A more revealing expression for the variance of $mr$ is derived in the following: 

$$\hat{j}(\mathbf{X},Y) = \mbox{arg max}_{j \neq Y}P_{\Theta}(h(\mathbf{X}, \Theta) = j)$$

so

\begin{align}
mr(\mathbf{X},Y) =& P_{\Theta}(h(\mathbf{X},\Theta = Y)) - P_{\Theta}(h(\mathbf{X},\Theta) = \hat{j}(\mathbf{X},Y)) \\
=& E_{\Theta}\left[I(h(\mathbf{X},\Theta = Y)) - I(h(\mathbf{X},\Theta) = \hat{j}(\mathbf{X},Y)\right]
\end{align}

---
class: middle
##2.2 Strength and correlation 


###Definition 2.2 The raw margin function is

$$rmg(\Theta, \mathbf{X},Y) = I(h(\mathbf{X},\Theta)=Y) - I(h(\mathbf{X},\Theta)=\hat{j}(\mathbf{X},Y)).$$

- Thus, $mr(\mathbf{X},Y)$ is the expectation of $rmg(\Theta, \mathbf{X},Y)$ with respect to $\Theta.$


- For any function $f$

$$\left[E_{\Theta} f(\Theta) \right]^2 = E_{\Theta, \Theta^{'}}f(\Theta )f(\Theta^{'})$$
holds where $\Theta, \Theta^{'}$ are independent with the same distribution, implying that

$$mr(\mathbf{X},Y)^2 =  E_{\Theta, \Theta^{'}}rmg(\Theta, \mathbf{X},Y)rmg(\Theta^{'}, \mathbf{X},Y)$$


---
class: middle
##2.2 Strength and correlation 

\begin{align}
Var_{\mathbf{X},Y}(mr) =& E_{\mathbf{X},Y}[mr^2] -  E^2_{\mathbf{X},Y}[mr]\\
=& E_{\mathbf{X},Y}[E_{\Theta, \Theta^{'}}[rmg(\Theta, \mathbf{X},Y)rmg(\Theta^{'}, \mathbf{X},Y)]] -  \\
& E_{\mathbf{X},Y}[E_{\Theta}[rmg(\Theta, \mathbf{X},Y)]]  E_{\mathbf{X},Y}[E_{\Theta^{'}}[rmg(\Theta^{'}, \mathbf{X},Y)]] \\
=& E_{\Theta, \Theta^{'}}[
E_{\mathbf{X},Y}[ rmg(\Theta, \mathbf{X},Y) rmg(\Theta^{'}, \mathbf{X},Y)] - E_{\mathbf{X},Y}[rmg(\Theta,\mathbf{X},Y)]
E_{\mathbf{X},Y}[rmg(\Theta^{'}, \mathbf{X},Y)]
] \\
=& E_{\Theta, \Theta^{'}}[ Cov_{\mathbf{X},Y}[ rmg(\Theta, \mathbf{X},Y) rmg(\Theta^{'}, \mathbf{X},Y)]    ] \\
=& E_{\Theta, \Theta^{'}}[\rho(\Theta, \Theta^{'}) sd(\Theta) sd(\Theta^{'})]
\end{align}

where $\rho(\Theta, \Theta^{'})$ is the correlation between $rmg(\Theta, \mathbf{X},Y)$ and $rmg(\Theta^{'}, \mathbf{X},Y)$ holding $\Theta$, $\Theta^{'}$ fixed and $sd(\Theta)$ is the standard deviation of $rmg(\Theta, \mathbf{X},Y)$ holding $\Theta$ fixed.



**Note: I am not sure. I have to double check !!!**

---
class: middle
##2.2 Strength and correlation    

Then,

\begin{align}
Var(mr) =&  E_{\Theta, \Theta^{'}}[\rho(\Theta, \Theta^{'}) sd(\Theta) sd(\Theta^{'})] \\
=& \frac{E_{\Theta, \Theta^{'}}[\rho(\Theta, \Theta^{'}) sd(\Theta) sd(\Theta^{'})] }{ E_{\Theta, \Theta^{'}}[sd(\Theta) sd(\Theta^{'})]}(E_{\Theta} sd(\Theta))^2  \\
=& \bar{\rho}(E_{\Theta} sd(\Theta))^2 \\
\leq&  \bar{\rho} E_{\Theta} Var(\Theta).
\end{align}

where $\bar{\rho} = \frac{E_{\Theta, \Theta^{'}}[\rho(\Theta, \Theta^{'}) sd(\Theta) sd(\Theta^{'})] }{ E_{\Theta, \Theta^{'}}[sd(\Theta) sd(\Theta^{'})]}.$ and
$Var(\Theta) = Var_{\mathbf{X},Y}(rmg(\Theta,\mathbf{X},Y)).$

---
class: middle
##2.2 Strength and correlation    



Write

\begin{align}
E_{\Theta}Var(\Theta) & \leq 
E_{\Theta}(E_{\mathbf{X},Y}rmg(\Theta,\mathbf{X},Y))^2 - s^2 \\
& \leq 1 - s^2.
\end{align}

We have

###Theorem 2.3 An upper bound for the generalization error is given by

$$PE^{*} \leq \frac{\bar{\rho}(1 - s^2)}{s^2}.$$


- It shows that the two ingredients involved in the generalization error for random forests are the strength of the individual classifiers in the forest, and the correlation between them in terms of the raw margin functions. 

- The $\frac{c}{s2} = \frac{\bar{\rho}}{s^2}$ ratio is the correlation divided by the square of the strength. 

- In understanding the functioning of random forests, this ratio will be a helpful guideâ€”the smaller it is, the better.


---
class: middle
##3. Using random features  

The resulting forests give accuracy that compare favorably with Adaboost. This class of procedures has desirable characteristics:

i.    Its accuracy is as good as Adaboost and sometimes better.

ii.   It's relatively robust to outliers and noise

iii.  It's faster than bagging or boosting.

iv.   It gives useful internal estimates of error, strength, correlation,
and variable importance.

v.    It's simple and easily parallelized.

---
class: middle
##3.1. Using out-of-bag estimates to monitor error, strength, and correlation

-  In my experiments with random forests, bagging is used in tandem with random feature selection.

- Each new training set is drawn, with replacement, from the original training set.
Then a tree is grown on the new training set using random feature selection. The trees grown are not pruned.

- Assume a method for constructing a classifier from any training set. Given a specific training set $T,$ form bootsrap training sets $T_k$ construct classifiers $h(\mathbf{x}, T_k)$ and let these vote to form the bagged predictor.

- For each $y$, $\mathbf{x}$ in the training set, aggregate the votes only over those classifiers for which $T_k$ dose not containing $y$, $\mathbf{x}.$

- Call this the out-of-bag classifier. Then the out-of-bag estimate for the generalization error is the error rate of the out-of-bag classifier on the training set. 

---
class: middle
##3.1. Using out-of-bag estimates to monitor error, strength, and correlation

- In each bootstrap training set, about one-third of the instances are left out. Therefore, the out-of-bag estimates are based on combining only about one-third as many classifiers as in the ongoing main combination. 

- Since the error rate decreases as the number of combinations increases, the out-of-bag estimates will tend to overestimate the current error rate. 

- To get unbiased out-of-bag estimates, it is necessary to run past the point where the test set error converges.

- But unlike cross-validation, where bias is present but its extent unknown, the
out-of-bag estimates are unbiased.

- Strength and correlation can also be estimated using out-of-bag methods. This gives internal estimates that are helpful in understanding classification accuracy and how to improve it.

---
class: middle
## 10. Exploring the random forest mechanism

- A forest of trees is impenetrable as far as simple interpretations of its mechanism go. 

- In some applications, analysis of medical experiments for example, it is critical to understand the interaction of variables that is providing the predictive accuracy.

- A start on this problem is made by using internal out-of-bag estimates, and verification by reruns using only selected variables.

- Suppose there are $M$ input variables. After each tree is constructed, the values of the $mth$ variable in the out-of-bag examples are randomly permuted and the out-of-bag data is run down the corresponding tree.

- The classification given for each $x_n$ that is out of bag is saved.

- This is repeated for $m = 1, 2, \ldots, M.$ . At the end of the run, the plurality of out-of-bag class votes for $\mathbf{x}_n$ with the $mth$ variable noised up is compared with the true class label of $\mathbf{x}_n$ to give a misclassification rate.

---
class: middle
##11. Random forests for regression

- Random forests for regression are formed by growing trees depending on a random vector $\Theta$ such that the tree predictor $h(\mathbf{X}, \Theta)$ takes on numerical values as opposed to class labels.

- The output values are numerical and we assume that the training set is independently drawn from the distribution of the random vector $Y, \mathbf{X}$.

- The mean-squared generalization error for any numerical predictor $h(\mathbf{X})$ is

$$E_{\mathbf{X},Y}(Y - h(\mathbf{X}))^2$$

- The random forest predictor is formed by taking the average over $k$ of the trees $\left\{ h(\mathbf{x},\Theta_k) \right\}.$ Similarly to the classification case, the following holds:


---
class: middle
##11. Random forests for regression


**Theorem 11.1. As the number of trees in the forest goes to infinity, almost surely,**

$$E_{\mathbf{X},Y}(Y - av_k h(\mathbf{X},\Theta_k))^2 \rightarrow E_{\mathbf{X},Y}(Y - E_{\Theta}h(\mathbf{X},\Theta))^2.$$
where $E_{\mathbf{X},Y}(Y - E_{\Theta}h(\mathbf{X},\Theta))^2$ is the generalization error of the forest $PE^*(forest).$

- Define the average generalization error of a tree as:

$$PE^{*}(tree) = E_{\Theta} E_{\mathbf{X},Y}(Y - h(\mathbf{X},\Theta))^2$$

---
class: middle
##11. Random forests for regression


**Theorem 11.2. Assume that for all** $\Theta,$ $EY = E_{\mathbf{X}}h(\mathbf{X},\Theta).$ **Then**

$$PE^{*}(forest) \leq \bar{\rho}PE^{*}(tree)$$
where $\bar{\rho}$ is the weighted correlation between the residuals $Y - h(\mathbf{X},\Theta)$ and $Y - h(\mathbf{X},\Theta)$ where $\Theta,\Theta^{'}$ are independent.

\begin{align}
PE^{*}(forest) =& E_{\mathbf{X},Y}[E_{\Theta}(Y - h(\mathbf{X}, \Theta))]^2 \\
=& E_{\Theta}E_{\Theta^{'}}E_{\mathbf{X},Y}(Y - h(\mathbf{X},\Theta))(Y - h(\mathbf{X},\Theta^{'})) \\
=& E_{\Theta}E_{\Theta^{'}}(\rho(\Theta,\Theta^{'})sd(\Theta)sd(\Theta^{'}))
\end{align}
where $sd(\Theta) = \sqrt{E_{\mathbf{X},Y}(Y - h(\mathbf{X},\Theta))^2}.$ 

Define the weighted correlation as:

$$\bar{\rho} = \frac{ E_{\Theta}E_{\Theta^{'}}(\rho(\Theta,\Theta^{'})sd(\Theta)sd(\Theta^{'}))}{(E_{\Theta}sd(\Theta))^2}.$$

Then 

$$PE^{*}(forest) = \bar{\rho}(E_{\Theta}sd(\Theta))^2 \leq \bar{\rho}PE^{*}(tree).$$

---
class: middle
##11. Random forests for regression


- Theorem 11.2 pinpoints the requirements for accurate regression forests: low correlation between residuals and low error trees.

- The random forest decreases the average error of the trees employed by the factor $\bar{\rho}.$

- The randomization employed needs to aim at low correlation.


