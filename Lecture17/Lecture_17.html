<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Lecture 17 - Classification and Regression Trees</title>
    <meta charset="utf-8" />
    <meta name="author" content="Material based on Lecture 21: Classification and Regression Tree (Department od Statistical - NCS) written by Wenbin Lu - Presented by Renato Rodrigues Silva" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Lecture 17 - Classification and Regression Trees
### Material based on Lecture 21: Classification and Regression Tree (Department od Statistical - NCS) written by Wenbin Lu - Presented by Renato Rodrigues Silva
### Federal University of Goias.
### (updated: 2020-10-10)

---

class: middle
##Outline

- Basic Ideas

- Examples

- Tree Construction Algorithm
  
    - Parameter Tunning
    - Choice of Impurity Measures
    - Missing Values
    
- How to use `rpart` package  


---
class: middle
##Classification tree - Overview


The tree is built by the following process: 

- First the single variable is found which best splits the data into two groups (‘best’ will be defined later). 

- The data is separated, and then this process is applied separately to each sub-group, and so on recursively until the subgroups either reach a minimum size (5 for this data) or until no improvement can be made.

- The resultant model is, with a certainty, too complex, and the question arises as it does with all stepwise procedures of when to stop. 

- The second stage of the procedure consists of using cross-validation to trim back the full tree. 

- A cross validated estimate of risk was computed for a nested set of sub trees.

- This final model was that sub tree with the lowest estimate of risk.


[Tutorial:](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf)


---
class: middle
##Example - Recursive Partitioning 


&lt;img src="Lecture_17_files/figure-html/Fig2.png" width="70%" align="center" /&gt;


[Source:](https://web.stanford.edu/~hastie/ElemStatLearn/)

[Video 1:](https://www.youtube.com/watch?v=p17C9q2M00Q&amp;list=PLD0F06AA0D2E8FFBA&amp;index=7)

[Video 2:](https://www.youtube.com/watch?v=S51plSJBC2g&amp;list=PLD0F06AA0D2E8FFBA&amp;index=10)


---
class: middle
##Example - Revival Data


- We are going to start with an example applied to the clinical area.

- An example is some preliminary data gathered at Stanford on the revival of cardiac arrest patients by paramedics. 


- The goal is to predict which patients can be successfully revived in the field based on fourteen variables.

-  Example: 
  -  Sex, 
  -  Age
  -  Time from attack to first care, etc.


---
class: middle
##Classification Tree: Revival data


&lt;img src="Lecture_17_files/figure-html/Fig1.png" width="100%" align="center" /&gt;

---
class: middle
##Classification Tree: Revival data

The resultant model separated the patients into four groups.

 - `\(X_1 =\)` initial heart rhythm

  `$$1 = \mbox{VF/VT;} \phantom{11} 2 = \mbox{EMD;} \phantom{11} 3 = \mbox{Asystole;} \phantom{11} 4 = \mbox{Other}.$$`  

- `\(X_2 =\)`  initial response to defibrillation
  `$$1 = \mbox{Improved;} \phantom{11} 2 = \mbox{No change;}\phantom{11} 3 = \mbox{Worse.}$$`  
  
- `\(X_3 =\)`  initial response to drugs
  `$$1 = \mbox{Improved;} \phantom{11} 2 = \mbox{No change;}\phantom{11} 3 = \mbox{Worse.}$$`  
  
  The other 11 variables did not appear in the final model.
  
[VF/VT:](https://nhcps.com/lesson/acls-cases-ventricular-fibrillation-pulseless-ventricular-tachycardia/)




---
class: middle
##Elements of a Tree: Nodes and Splits


`\(T:\)` a collection of nodes `\((t)\)` and aplits `\((s).\)`

- root node

- terminal node (leaf node)
    - Each leaf node is assigned with a class fit.

- parent node

- child node
  - left node `\((t_L)\)`
  - right node `\((t_R)\)`
  
- The size of tree
  `\(|T|\)` = number of terminal nodes in `\(T\)`

[Lecture NCS:](https://www4.stat.ncsu.edu/~lu/ST7901/lecture%20notes/2019Lect21_tree.pdf)

---
class: middle
##Growing a tree

- Split process
  
  - choose splitting variables and split points;
  - goodness of split criterion `\(\phi(s,t)\)` needed to 
  evaluate any split `\(s\)` of any node `\(t\)`.
  
- Partition process

  - partition the data intro two resulting regions and 
  repeat the splitting process on each region
  
  - declare a node is terminal or to continue splitting it;
  need a stop-splitting rule.
  
- Pruning Process

  - collapse some branches back together

---
class: middle
##Splitting Process

- Each split depends on the values of only one unique variable `\(X_j\)`

- If `\(X_j\)` is continuous, the splitting question is 

`$$\left\{Is \phantom{11} X_j \leq s  \right\}?$$`

for all real values `\(s\)`.

  - Since the training data set is finite, there are only finitely many distinct splits generated by the question.

- If `\(X_j\)` is categorical taking values `\(\left\{1, \ldots, M \right\},\)` the splitting question is

`$$\left\{Is \phantom{11} X_j \in A  \right\}?$$`

`\(A\)` is a subset of `\(\left\{1, \ldots, M \right\}.\)`

---
class: middle

##Choose Best Split

- For the splitting variable `\(j\)` and split points `\(s\)`,
define the pair of half planes

`\(R_1(j,s) = \left\{ \mathbf{X}|X_j \leq s\right\}\)` and 
`\(R_2(j,s) = \left\{ \mathbf{X}|X_j &gt; s\right\}.\)`

- Each split produces two subnodes.

- We scan through all the inputs and all the possible split 
quickly and determine the best pair `\((j,s)\)` yielding two most "pure" nodes, i.e., 

`$$\mbox{min}_{j,s}\left[\phi_{R_1} + \phi_{R_2}\right].$$`
here `\(\phi_{R_m}\)` is the some purity measure of node `\(R_m\)`
for `\(m = 1, 2.\)`

---
class: middle

##Purity Measure of Multiclass Problems


In multiclass classification problems,

- Assume each subnode `\(R_m\)` contains `\(N_m\)` observations, let

`$$\hat{p}_{mk} = Pr(k|m) = \frac{1}{N_m} \sum_{\mathbf{x}_i \in R^m} I(y_i = k),$$`
which is the proportion of class `\(k\)` observations in node `\(m\)`.

- What is the characteristic of a purity (or impurity) function?

- A node is more pure if one class dominates the node than if multiple classes equally present in the node.

---
class: middle

##Impurity Functions

- An impurity function is a function `\(\phi\)`
defined on the set

`$$\left\{(p_1, \ldots, p_K): p_k \geq 0, 
k = 1, \ldots, K, \sum_{k=1}^K p_k = 1 \right\},$$`

satisfying

- `\(\phi\)` is a maximum only at the points `\((\frac{1}{K}, \ldots, \frac{1}{K}).\)` (most impure).

- `\(\phi\)` gets minimum only at $\left(1, 0, \ldots, 0\right),
\left(0, 1, \ldots, 0\right)$ (most pure case).

- `\(\phi\)` is a symmetric function of `\((p_1, \ldots, p_K).\)`

---
class: middle

##Popular Impurity Functions

- Misclassification error: `\(\phi = 1 - \mbox{max}_{k=1,\ldots,K} p_k.\)`

- Gini index: `\(\phi = \sum_{k=1}^K p_k(1 - p_k).\)`

- Cross-entropy (deviance): `\(\phi = - \sum_{k=1}^K p_k \log p_k.\)` (Default: 0\log(0) = 0.)


For two classes, if `\(p\)` is the proportion in the second class, these three measures are `\(1 − max(p,1 − p)\)`, `\(2p(1 − p)\)` and −$p\log p − (1 − p)\log(1−p)$, respectively


---
class: middle
##Node impurity measure:


&lt;img src="Lecture_17_files/figure-html/Fig3.png" width="100%" align="center" /&gt;


---
class: middle
##Estimated Node impurity measure:

- For each node `\(R_m,\)` its impurity `\(\phi_{R_m}\)` can 
be estimated as following:

- Misclassification error:

`$$\phi_{R_m} = \frac{1}{N_m} 
\sum_{\mathbf{x}_i \in R^m } I(y_i \neq k(m)) = 
1 - \hat{p}_{mk (m)}$$`

- Gini index:

`$$\phi_{R_m} = \sum_{k \neq k^{'}} \hat{p}_{mk} \hat{p}_{mk^{'}} = \sum_{k=1}^K \hat{p}_{mk}(1 -\hat{p}_{mk} )$$`

- Cross-entropy (deviance)

`$$\phi_{R_m} = - \sum_{k=1}^K \hat{p}_{mk} \log \hat{p}_{mk}.$$`


---
class: middle
##Total Impurity of a Tree

- Assume we use the impurity measure `\(\phi.\)` 
Given a tree `\(T\)` containing `\(m\)` nodes `\(R_m^{'}\)`s.

- the size of tree is `\(|T|.\)`

- the sample size of each node `\(R_m\)` is `\(N_m.\)`

- the impurity of for each node is `\(\phi_{R_m}.\)`

- The total impurity of the tree is 

`$$\sum_{m=1}^{|T|}N_m \phi_{R_m}.$$`

---
class: middle

##Partition  Process

- We repeat the binary partition recursively until a tree is large enough.

    - A very large tree might overfit the data

    - A small tree might not capture the important structure

    - The optimal tree size should be adaptively chosen from the data.

- When should we stop the partition?

    - One possible approach is to split tree nodes ONLY IF the decrease in impurity due to splits exceeds some threshold

    - Drawback: A seemingly worthless split might lead to a very good split below it (short-sighted)


**Preferred approach**: grow a large tree `\(T_0\)`, stopping the splitting process only when some minimum node size (say 5) is reached.


---
class: middle

##Pruning  Process

- We use the weakest link pruning procedure:

- successively collapse the internal node that produces
the smallest per-node increase in `\(\sum_{m=1}^{|T|} N_m Q_m(T);\)` here `\(Q_m(T)\)` is the impurity measure of node `\(m\)`.

- continue until we produce the single-node (root) tree.

This gives a finite sequence of subtrees. For each subtree T, measure its cost complexity by

`$$C_{\alpha}(T) = \sum_{m=1}^{|T|} N_m Q_m(T) + \alpha |T|.$$`
where `\(m\)` run over all the terminal nodes in `\(T,\)` and `\(\alpha\)` governs a tradeoff between tree size `\(|T|\)` and its goodness of data. 

- Large `\(\alpha\)` results in smaller trees;

- Small `\(\alpha\)` results in large trees.

---
class: middle

##Parameter Tuning for Tree

-  For each `\(\alpha,\)` there is a unique smallest 
subtree `\(T_{\alpha}\)` that minimizes 
`\(C_{\alpha}(T).\)`

- The sequence of subtrees obtained by pruning under 
the weakest link, must contain `\(T_{\alpha}.\)` (Breiman (1984) and Ripley (1996)).

In pratice,

- One uses the five or ten-fold cross validation to choose the best `\(\alpha.\)`


---
class: middle

##Missing Values

- Missing values for some variables are often encountered in high dimensional data, for example, in gene expression data.

- If each variable has 5% chance missing independently. With 50 variables, probability of missing some variable is as high as 92.3%.

###Traditional approaches:

- Discard observations with missing values for example, depletion of training set.

- **Impute** the missing values for example, via the mean over complete observations, i.e, R package `mice`.


---
class: middle

##How Do Tree-Methods Handle Missing Values?

- Others approaches:

- For categorical variables, create the category “missing”,

    - Helps to discover that observations with missing values behave differently than those complete observations.

- Construct surrogate variables besides the best splitting variable
    - If the primary splitting variable is missing, use surrogate splits in order.
  
  
---
##Surrogate - variables


- Once a splitting variable and a split point for it have been decided, what is to be done with observations missing that variable?


- One approach is to estimate the missing datum using the other independent variables; rpart uses a variation of this to define surrogate variables.

- As an example, assume that the split `\((age &lt;40, age \geq 40)\)` has been chosen. 

- The surrogate variables are found by re-applying the partitioning algorithm (without recursion) to predict the two categories `\((age &lt;40)\)` vs. `\((age \geq 40)\)` using the other independent variables.


[Other explanations:](https://stats.stackexchange.com/questions/171574/meaning-of-surrogate-split)    
    
---
class: middle
##Regression Trees

- Suppose we have a partition into `\(M\)` regions `\(R_1, \ldots, R_M.\)` Consider a piece-wise constant model

`\(f(x) = \sum_{m=1}^M c_m I(x \in R_m).\)`

- The minimizers of `\(\sum_{i}(y_i - f(x_i))^2\)` 
are given by

`$$\hat{c}_m = \frac{1}{N_m}\sum_{x_i \in R_m}y_i,$$`

- Seek the splitting variable `\(j\)` and split point `\(s\)` that solve

`$$\mbox{min}_{j,s}\left[\mbox{min}_{c_1} \sum_{x_i \in R_1(j,s)} (y_i - c_1)^2  + \mbox{min}_{c_1} \sum_{x_i \in R_1(j,s)} (y_i - c_2)^2 \right].$$`

- Define impurity measure: `\(Q_m(T) = N_m^{-1} \sum_{x_i \in R_m}(y_i - \hat{c}_m)^2.\)`


[Video 1:](https://www.youtube.com/watch?v=zvUOpbgtW3c&amp;list=PLD0F06AA0D2E8FFBA&amp;index=8)

[Video 2:](https://www.youtube.com/watch?v=_RxqyvRK0Rw&amp;list=PLD0F06AA0D2E8FFBA&amp;index=9)
    
---
class: middle

##Example  - Stage C prostate cancer 

- This  example is based on a data set of 146 stage C prostate cancer patients. 

- The main clinical endpoint of interest is whether the disease recurs after initial surgical removal of the prostate, and the time interval to that progression (if any).

- The endpoint in this example is status, which takes on the value 1 if the disease has progressed and 0 if not.


- The variables in the data set are


- **pgtime:** time to progression, or last follow-up free of progression. 
- **pgstat:** status at last follow-up (1=progressed, 0=censored).
- **age:** age at diagnosis
- **eet:** early endocrine therapy (1=no, 0=yes)
- **ploidy:** diploid/tetraploid/anueploid DNA pattern
- **g2:** % of cells in `\(G_2\)` phase
- **grade:** tumor grade (1-4)
- **gleason:** Gleason grade (3-10)





---
class: middle

##R code


```r
library(rpart)

progstat &lt;- factor(stagec$pgstat, levels = 0:1, labels = c("No", "Prog"))

cfit &lt;- rpart(progstat ~ age + eet + g2 + grade + gleason + ploidy, data = stagec, method = 'class')

print(cfit)
plot(cfit)
text(cfit)

printcp(cfit)
plotcp(cfit)
```


---
class: middle

##R code


```
## n= 146 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 146 54 No (0.6301370 0.3698630)  
##    2) grade&lt; 2.5 61  9 No (0.8524590 0.1475410) *
##    3) grade&gt;=2.5 85 40 Prog (0.4705882 0.5294118)  
##      6) g2&lt; 13.2 40 17 No (0.5750000 0.4250000)  
##       12) ploidy=diploid,tetraploid 31 11 No (0.6451613 0.3548387)  
##         24) g2&gt;=11.845 7  1 No (0.8571429 0.1428571) *
##         25) g2&lt; 11.845 24 10 No (0.5833333 0.4166667)  
##           50) g2&lt; 11.005 17  5 No (0.7058824 0.2941176) *
##           51) g2&gt;=11.005 7  2 Prog (0.2857143 0.7142857) *
##       13) ploidy=aneuploid 9  3 Prog (0.3333333 0.6666667) *
##      7) g2&gt;=13.2 45 17 Prog (0.3777778 0.6222222)  
##       14) g2&gt;=17.91 22  8 No (0.6363636 0.3636364)  
##         28) age&gt;=62.5 15  4 No (0.7333333 0.2666667) *
##         29) age&lt; 62.5 7  3 Prog (0.4285714 0.5714286) *
##       15) g2&lt; 17.91 23  3 Prog (0.1304348 0.8695652) *
```


---
class: middle

##R code

![](Lecture_17_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;


---
class: middle

##Results from CV


```
## 
## Classification tree:
## rpart(formula = progstat ~ age + eet + g2 + grade + gleason + 
##     ploidy, data = stagec, method = "class")
## 
## Variables actually used in tree construction:
## [1] age    g2     grade  ploidy
## 
## Root node error: 54/146 = 0.36986
## 
## n= 146 
## 
##         CP nsplit rel error  xerror    xstd
## 1 0.104938      0   1.00000 1.00000 0.10802
## 2 0.055556      3   0.68519 0.87037 0.10454
## 3 0.027778      4   0.62963 0.90741 0.10566
## 4 0.018519      6   0.57407 0.92593 0.10618
## 5 0.010000      7   0.55556 0.98148 0.10760
```

---
class: middle

##Interpretation 

- The relative error measures the error of classification of the training data obtained through the model (`rel.error`). 

- The term labeled xerror is the prediction error obtained by cross-validation (`xerror`) and the corresponding standard error is labeled `xstd`.

---
class: middle

####Results from CV

![](Lecture_17_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;


---
class: middle

##R code


```r
set.seed(42)
cfit.prune = prune(cfit, cp = cfit$cptable[which.min(cfit$cptable[,"xerror"]),"CP"]
)
```


---
class: middle

##R code

![](Lecture_17_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;

---
class: middle

##R code


```
## 
## Classification tree:
## rpart(formula = progstat ~ age + eet + g2 + grade + gleason + 
##     ploidy, data = stagec, method = "class")
## 
## Variables actually used in tree construction:
## [1] g2    grade
## 
## Root node error: 54/146 = 0.36986
## 
## n= 146 
## 
##         CP nsplit rel error  xerror    xstd
## 1 0.104938      0   1.00000 1.00000 0.10802
## 2 0.055556      3   0.68519 0.87037 0.10454
```

---
class: middle

##R code

![](Lecture_17_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;


---
class: middle
###Regression Tree - Automobile Data from 'Consumer Reports' 1990

###Description

- The `cu.summary` data frame has 117 rows and 5 columns, giving data on makes of cars taken from the April, 1990 issue of Consumer Reports.



###Variables 

**Price:** - a numeric vector giving the list price in US dollars of a standard model.

**Country:** - country of origin, a factor with levels Brazil, England, France, Germany, Japan, Japan/USA, Korea, Mexico, Sweden and USA

**Reliability:** - an ordered factor with levels Much worse &lt; worse &lt; average &lt; better &lt; Much better.

**Mileage:** - fuel consumption miles per US gallon, as tested.

**Type:** - a factor with levels Compact Large Medium Small Sporty Van.

---
class: middle

##R code


```r
set.seed(42)
# grow tree
rfit &lt;- rpart(Mileage~., method="anova", data=cu.summary)

print(rfit)
plot(rfit)
text(rfit)

printcp(rfit)
plotcp(rfit)
```


---
class: middle

##R code


```
## n=60 (57 observations deleted due to missingness)
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 60 1354.58300 24.58333  
##    2) Price&gt;=9446.5 48  407.91670 22.70833  
##      4) Type=Large,Medium,Van 23   66.86957 20.69565  
##        8) Type=Large,Van 10   22.10000 19.30000 *
##        9) Type=Medium 13   10.30769 21.76923 *
##      5) Type=Compact,Small,Sporty 25  162.16000 24.56000  
##       10) Price&gt;=11484.5 14  107.71430 23.85714 *
##       11) Price&lt; 11484.5 11   38.72727 25.45455 *
##    3) Price&lt; 9446.5 12  102.91670 32.08333 *
```


---
class: middle

##R code

![](Lecture_17_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;






---
class: middle

##R code


```
## 
## Regression tree:
## rpart(formula = Mileage ~ ., data = cu.summary, method = "anova")
## 
## Variables actually used in tree construction:
## [1] Price Type 
## 
## Root node error: 1354.6/60 = 22.576
## 
## n=60 (57 observations deleted due to missingness)
## 
##         CP nsplit rel error  xerror     xstd
## 1 0.622885      0   1.00000 1.02414 0.175654
## 2 0.132061      1   0.37711 0.52699 0.102180
## 3 0.025441      2   0.24505 0.37232 0.082402
## 4 0.011604      3   0.21961 0.36018 0.076625
## 5 0.010000      4   0.20801 0.39997 0.084078
```


---
class: middle

##R code

![](Lecture_17_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;


---
class: middle

##R code


```r
set.seed(42)
rfit.prune = prune(rfit, cp = rfit$cptable[which.min(rfit$cptable[,"xerror"]),"CP"]
)
```


---
class: middle

##R code

![](Lecture_17_files/figure-html/unnamed-chunk-16-1.png)&lt;!-- --&gt;

---
class: middle

##R code


```
## 
## Regression tree:
## rpart(formula = Mileage ~ ., data = cu.summary, method = "anova")
## 
## Variables actually used in tree construction:
## [1] Price Type 
## 
## Root node error: 1354.6/60 = 22.576
## 
## n=60 (57 observations deleted due to missingness)
## 
##         CP nsplit rel error  xerror     xstd
## 1 0.622885      0   1.00000 1.02414 0.175654
## 2 0.132061      1   0.37711 0.52699 0.102180
## 3 0.025441      2   0.24505 0.37232 0.082402
## 4 0.011604      3   0.21961 0.36018 0.076625
## 5 0.010000      4   0.20801 0.39997 0.084078
```

---
class: middle

##R code

![](Lecture_17_files/figure-html/unnamed-chunk-18-1.png)&lt;!-- --&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
