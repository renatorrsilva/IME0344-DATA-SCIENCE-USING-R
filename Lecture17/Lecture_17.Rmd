---
title: "Lecture 17 - Classification and Regression Trees"
author: "Material based on Lecture 21: Classification and Regression Tree (Department od Statistical  - NCS) written by Wenbin Lu - Presented by Renato Rodrigues Silva"
institute: "Federal University of Goias."
date: "(updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false


---
class: middle
##Outline

- Basic Ideas

- Examples

- Tree Construction Algorithm
  
    - Parameter Tunning
    - Choice of Impurity Measures
    - Missing Values
    
- How to use `rpart` package  


---
class: middle
##Classification tree - Overview


The tree is built by the following process: 

- First the single variable is found which best splits the data into two groups (‘best’ will be defined later). 

- The data is separated, and then this process is applied separately to each sub-group, and so on recursively until the subgroups either reach a minimum size (5 for this data) or until no improvement can be made.

- The resultant model is, with a certainty, too complex, and the question arises as it does with all stepwise procedures of when to stop. 

- The second stage of the procedure consists of using cross-validation to trim back the full tree. 

- A cross validated estimate of risk was computed for a nested set of sub trees.

- This final model was that sub tree with the lowest estimate of risk.


[Tutorial:](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf)


---
class: middle
##Example - Recursive Partitioning 


<img src="Lecture_17_files/figure-html/Fig2.png" width="70%" align="center" />


[Source:](https://web.stanford.edu/~hastie/ElemStatLearn/)

[Video 1:](https://www.youtube.com/watch?v=p17C9q2M00Q&list=PLD0F06AA0D2E8FFBA&index=7)

[Video 2:](https://www.youtube.com/watch?v=S51plSJBC2g&list=PLD0F06AA0D2E8FFBA&index=10)


---
class: middle
##Example - Revival Data


- We are going to start with an example applied to the clinical area.

- An example is some preliminary data gathered at Stanford on the revival of cardiac arrest patients by paramedics. 


- The goal is to predict which patients can be successfully revived in the field based on fourteen variables.

-  Example: 
  -  Sex, 
  -  Age
  -  Time from attack to first care, etc.


---
class: middle
##Classification Tree: Revival data


<img src="Lecture_17_files/figure-html/Fig1.png" width="100%" align="center" />

---
class: middle
##Classification Tree: Revival data

The resultant model separated the patients into four groups.

 - $X_1 =$ initial heart rhythm

  $$1 = \mbox{VF/VT;} \phantom{11} 2 = \mbox{EMD;} \phantom{11} 3 = \mbox{Asystole;} \phantom{11} 4 = \mbox{Other}.$$  

- $X_2 =$  initial response to defibrillation
  $$1 = \mbox{Improved;} \phantom{11} 2 = \mbox{No change;}\phantom{11} 3 = \mbox{Worse.}$$  
  
- $X_3 =$  initial response to drugs
  $$1 = \mbox{Improved;} \phantom{11} 2 = \mbox{No change;}\phantom{11} 3 = \mbox{Worse.}$$  
  
  The other 11 variables did not appear in the final model.
  
[VF/VT:](https://nhcps.com/lesson/acls-cases-ventricular-fibrillation-pulseless-ventricular-tachycardia/)




---
class: middle
##Elements of a Tree: Nodes and Splits


$T:$ a collection of nodes $(t)$ and splits $(s).$

- root node

- terminal node (leaf node)
    - Each leaf node is assigned with a class fit.

- parent node

- child node
  - left node $(t_L)$
  - right node $(t_R)$
  
- The size of tree
  $|T|$ = number of terminal nodes in $T$

[Lecture NCS:](https://www4.stat.ncsu.edu/~lu/ST7901/lecture%20notes/2019Lect21_tree.pdf)

---
class: middle
##Growing a tree

- Split process
  
  - choose splitting variables and split points;
  - goodness of split criterion $\phi(s,t)$ needed to 
  evaluate any split $s$ of any node $t$.
  
- Partition process

  - partition the data intro two resulting regions and 
  repeat the splitting process on each region
  
  - declare a node is terminal or to continue splitting it;
  need a stop-splitting rule.
  
- Pruning Process

  - collapse some branches back together

---
class: middle
##Splitting Process

- Each split depends on the values of only one unique variable $X_j$

- If $X_j$ is continuous, the splitting question is 

$$\left\{Is \phantom{11} X_j \leq s  \right\}?$$

for all real values $s$.

  - Since the training data set is finite, there are only finitely many distinct splits generated by the question.

- If $X_j$ is categorical taking values $\left\{1, \ldots, M \right\},$ the splitting question is

$$\left\{Is \phantom{11} X_j \in A  \right\}?$$

$A$ is a subset of $\left\{1, \ldots, M \right\}.$

---
class: middle

##Choose Best Split

- For the splitting variable $j$ and split points $s$,
define the pair of half planes

$R_1(j,s) = \left\{ \mathbf{X}|X_j \leq s\right\}$ and 
$R_2(j,s) = \left\{ \mathbf{X}|X_j > s\right\}.$

- Each split produces two subnodes.

- We scan through all the inputs and all the possible split 
quickly and determine the best pair $(j,s)$ yielding two most "pure" nodes, i.e., 

$$\mbox{min}_{j,s}\left[\phi_{R_1} + \phi_{R_2}\right].$$
here $\phi_{R_m}$ is the some purity measure of node $R_m$
for $m = 1, 2.$

---
class: middle

##Purity Measure of Multiclass Problems


In multiclass classification problems,

- Assume each subnode $R_m$ contains $N_m$ observations, let

$$\hat{p}_{mk} = Pr(k|m) = \frac{1}{N_m} \sum_{\mathbf{x}_i \in R^m} I(y_i = k),$$
which is the proportion of class $k$ observations in node $m$.

- What is the characteristic of a purity (or impurity) function?

- A node is more pure if one class dominates the node than if multiple classes equally present in the node.

---
class: middle

##Impurity Functions

- An impurity function is a function $\phi$
defined on the set

$$\left\{(p_1, \ldots, p_K): p_k \geq 0, 
k = 1, \ldots, K, \sum_{k=1}^K p_k = 1 \right\},$$

satisfying

- $\phi$ is a maximum only at the points $(\frac{1}{K}, \ldots, \frac{1}{K}).$ (most impure).

- $\phi$ gets minimum only at $\left(1, 0, \ldots, 0\right),
\left(0, 1, \ldots, 0\right)$ (most pure case).

- $\phi$ is a symmetric function of $(p_1, \ldots, p_K).$

---
class: middle

##Popular Impurity Functions

- Misclassification error: $\phi = 1 - \mbox{max}_{k=1,\ldots,K} p_k.$

- Gini index: $\phi = \sum_{k=1}^K p_k(1 - p_k).$

- Cross-entropy (deviance): $\phi = - \sum_{k=1}^K p_k \log p_k.$ (Default: 0\log(0) = 0.)


For two classes, if $p$ is the proportion in the second class, these three measures are $1 − max(p,1 − p)$, $2p(1 − p)$ and −$p\log p − (1 − p)\log(1−p)$, respectively


---
class: middle
##Node impurity measure:


<img src="Lecture_17_files/figure-html/Fig3.png" width="100%" align="center" />


---
class: middle
##Estimated Node impurity measure:

- For each node $R_m,$ its impurity $\phi_{R_m}$ can 
be estimated as following:

- Misclassification error:

$$\phi_{R_m} = \frac{1}{N_m} 
\sum_{\mathbf{x}_i \in R^m } I(y_i \neq k(m)) = 
1 - \hat{p}_{mk (m)}$$

- Gini index:

$$\phi_{R_m} = \sum_{k \neq k^{'}} \hat{p}_{mk} \hat{p}_{mk^{'}} = \sum_{k=1}^K \hat{p}_{mk}(1 -\hat{p}_{mk} )$$

- Cross-entropy (deviance)

$$\phi_{R_m} = - \sum_{k=1}^K \hat{p}_{mk} \log \hat{p}_{mk}.$$


---
class: middle
##Total Impurity of a Tree

- Assume we use the impurity measure $\phi.$ 
Given a tree $T$ containing $m$ nodes $R_m^{'}$s.

- the size of tree is $|T|.$

- the sample size of each node $R_m$ is $N_m.$

- the impurity of for each node is $\phi_{R_m}.$

- The total impurity of the tree is 

$$\sum_{m=1}^{|T|}N_m \phi_{R_m}.$$

---
class: middle

##Partition  Process

- We repeat the binary partition recursively until a tree is large enough.

    - A very large tree might overfit the data

    - A small tree might not capture the important structure

    - The optimal tree size should be adaptively chosen from the data.

- When should we stop the partition?

    - One possible approach is to split tree nodes ONLY IF the decrease in impurity due to splits exceeds some threshold

    - Drawback: A seemingly worthless split might lead to a very good split below it (short-sighted)


**Preferred approach**: grow a large tree $T_0$, stopping the splitting process only when some minimum node size (say 5) is reached.


---
class: middle

##Pruning  Process

- We use the weakest link pruning procedure:

- successively collapse the internal node that produces
the smallest per-node increase in $\sum_{m=1}^{|T|} N_m Q_m(T);$ here $Q_m(T)$ is the impurity measure of node $m$.

- continue until we produce the single-node (root) tree.

This gives a finite sequence of subtrees. For each subtree T, measure its cost complexity by

$$C_{\alpha}(T) = \sum_{m=1}^{|T|} N_m Q_m(T) + \alpha |T|.$$
where $m$ run over all the terminal nodes in $T,$ and $\alpha$ governs a tradeoff between tree size $|T|$ and its goodness of data. 

- Large $\alpha$ results in smaller trees;

- Small $\alpha$ results in large trees.

---
class: middle

##Parameter Tuning for Tree

-  For each $\alpha,$ there is a unique smallest 
subtree $T_{\alpha}$ that minimizes 
$C_{\alpha}(T).$

- The sequence of subtrees obtained by pruning under 
the weakest link, must contain $T_{\alpha}.$ (Breiman (1984) and Ripley (1996)).

In pratice,

- One uses the five or ten-fold cross validation to choose the best $\alpha.$


---
class: middle

##Missing Values

- Missing values for some variables are often encountered in high dimensional data, for example, in gene expression data.

- If each variable has 5% chance missing independently. With 50 variables, probability of missing some variable is as high as 92.3%.

###Traditional approaches:

- Discard observations with missing values for example, depletion of training set.

- **Impute** the missing values for example, via the mean over complete observations, i.e, R package `mice`.


---
class: middle

##How Do Tree-Methods Handle Missing Values?

- Others approaches:

- For categorical variables, create the category “missing”,

    - Helps to discover that observations with missing values behave differently than those complete observations.

- Construct surrogate variables besides the best splitting variable
    - If the primary splitting variable is missing, use surrogate splits in order.
  
  
---
##Surrogate - variables


- Once a splitting variable and a split point for it have been decided, what is to be done with observations missing that variable?


- One approach is to estimate the missing datum using the other independent variables; rpart uses a variation of this to define surrogate variables.

- As an example, assume that the split $(age <40, age \geq 40)$ has been chosen. 

- The surrogate variables are found by re-applying the partitioning algorithm (without recursion) to predict the two categories $(age <40)$ vs. $(age \geq 40)$ using the other independent variables.


[Other explanations:](https://stats.stackexchange.com/questions/171574/meaning-of-surrogate-split)    
    
---
class: middle
##Regression Trees

- Suppose we have a partition into $M$ regions $R_1, \ldots, R_M.$ Consider a piece-wise constant model

$f(x) = \sum_{m=1}^M c_m I(x \in R_m).$

- The minimizers of $\sum_{i}(y_i - f(x_i))^2$ 
are given by

$$\hat{c}_m = \frac{1}{N_m}\sum_{x_i \in R_m}y_i,$$

- Seek the splitting variable $j$ and split point $s$ that solve

$$\mbox{min}_{j,s}\left[\mbox{min}_{c_1} \sum_{x_i \in R_1(j,s)} (y_i - c_1)^2  + \mbox{min}_{c_1} \sum_{x_i \in R_1(j,s)} (y_i - c_2)^2 \right].$$

- Define impurity measure: $Q_m(T) = N_m^{-1} \sum_{x_i \in R_m}(y_i - \hat{c}_m)^2.$


[Video 1:](https://www.youtube.com/watch?v=zvUOpbgtW3c&list=PLD0F06AA0D2E8FFBA&index=8)

[Video 2:](https://www.youtube.com/watch?v=_RxqyvRK0Rw&list=PLD0F06AA0D2E8FFBA&index=9)
    
---
class: middle

##Example  - Stage C prostate cancer 

- This  example is based on a data set of 146 stage C prostate cancer patients. 

- The main clinical endpoint of interest is whether the disease recurs after initial surgical removal of the prostate, and the time interval to that progression (if any).

- The endpoint in this example is status, which takes on the value 1 if the disease has progressed and 0 if not.


- The variables in the data set are


- **pgtime:** time to progression, or last follow-up free of progression. 
- **pgstat:** status at last follow-up (1=progressed, 0=censored).
- **age:** age at diagnosis
- **eet:** early endocrine therapy (1=no, 0=yes)
- **ploidy:** diploid/tetraploid/anueploid DNA pattern
- **g2:** % of cells in $G_2$ phase
- **grade:** tumor grade (1-4)
- **gleason:** Gleason grade (3-10)





---
class: middle

##R code

```{r, warning=TRUE, message=TRUE, eval = FALSE}

library(rpart)

progstat <- factor(stagec$pgstat, levels = 0:1, labels = c("No", "Prog"))

cfit <- rpart(progstat ~ age + eet + g2 + grade + gleason + ploidy, data = stagec, method = 'class')

print(cfit)
plot(cfit)
text(cfit)

printcp(cfit)
plotcp(cfit)

```


---
class: middle

##R code

```{r, warning=TRUE, message=TRUE, echo = FALSE}

library(rpart)

progstat <- factor(stagec$pgstat, levels = 0:1, labels = c("No", "Prog"))

set.seed(42)
cfit <- rpart(progstat ~ age + eet + g2 + grade + gleason + ploidy, data = stagec, method = 'class')


print(cfit)

```


---
class: middle

##R code

```{r, warning=TRUE, message=TRUE, echo = FALSE}

plot(cfit)
text(cfit)


```


---
class: middle

##Results from CV

```{r, warning=TRUE, message=TRUE, echo = FALSE}

printcp(cfit)

```

---
class: middle

##Interpretation 

- The relative error measures the error of classification of the training data obtained through the model (`rel.error`). 

- The term labeled xerror is the prediction error obtained by cross-validation (`xerror`) and the corresponding standard error is labeled `xstd`.

---
class: middle

####Results from CV

```{r, warning=TRUE, message=TRUE, echo = FALSE}


plotcp(cfit)

```


---
class: middle

##R code

```{r, warning=TRUE, message=TRUE, eval = FALSE}
set.seed(42)
cfit.prune = prune(cfit, cp = cfit$cptable[which.min(cfit$cptable[,"xerror"]),"CP"]
)

```


---
class: middle

##R code

```{r, warning=TRUE, message=TRUE, echo = FALSE}
set.seed(42)
cfit.prune = prune(cfit, cp = cfit$cptable[which.min(cfit$cptable[,"xerror"]),"CP"]
)

plot(cfit.prune)
text(cfit.prune)

```

---
class: middle

##R code

```{r, warning=TRUE, message=TRUE, echo = FALSE}


printcp(cfit.prune)

```

---
class: middle

##R code

```{r, warning=TRUE, message=TRUE, echo = FALSE}


plotcp(cfit.prune)

```


---
class: middle
###Regression Tree - Automobile Data from 'Consumer Reports' 1990

###Description

- The `cu.summary` data frame has 117 rows and 5 columns, giving data on makes of cars taken from the April, 1990 issue of Consumer Reports.



###Variables 

**Price:** - a numeric vector giving the list price in US dollars of a standard model.

**Country:** - country of origin, a factor with levels Brazil, England, France, Germany, Japan, Japan/USA, Korea, Mexico, Sweden and USA

**Reliability:** - an ordered factor with levels Much worse < worse < average < better < Much better.

**Mileage:** - fuel consumption miles per US gallon, as tested.

**Type:** - a factor with levels Compact Large Medium Small Sporty Van.

---
class: middle

##R code

```{r, warning=TRUE, message=TRUE, eval = FALSE}

set.seed(42)
# grow tree
rfit <- rpart(Mileage~., method="anova", data=cu.summary)

print(rfit)
plot(rfit)
text(rfit)

printcp(rfit)
plotcp(rfit)


```


---
class: middle

##R code

```{r, warning=TRUE, message=TRUE, echo = FALSE}

set.seed(42)
# grow tree
rfit <- rpart(Mileage~., method="anova", data=cu.summary)

print(rfit)


```


---
class: middle

##R code

```{r, warning=TRUE, message=TRUE, echo = FALSE}


plot(rfit)
text(rfit)
```






---
class: middle

##R code

```{r, warning=TRUE, message=TRUE, echo = FALSE}

printcp(rfit)

```


---
class: middle

##R code

```{r, warning=TRUE, message=TRUE, echo = FALSE}

plotcp(rfit)

```


---
class: middle

##R code

```{r, warning=TRUE, message=TRUE, eval = FALSE}
set.seed(42)
rfit.prune = prune(rfit, cp = rfit$cptable[which.min(rfit$cptable[,"xerror"]),"CP"]
)

```


---
class: middle

##R code

```{r, warning=TRUE, message=TRUE, echo = FALSE}
set.seed(42)
rfit.prune = prune(rfit, cp = rfit$rptable[which.min(rfit$cptable[,"xerror"]),"CP"]
)

plot(rfit.prune)
text(rfit.prune)

```

---
class: middle

##R code

```{r, warning=TRUE, message=TRUE, echo = FALSE}


printcp(rfit.prune)

```

---
class: middle

##R code

```{r, warning=TRUE, message=TRUE, echo = FALSE}


plotcp(rfit.prune)

```


