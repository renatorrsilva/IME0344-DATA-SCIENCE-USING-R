<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Lecture 24 - Deep Learning - part II</title>
    <meta charset="utf-8" />
    <meta name="author" content="Material based on An overview of gradient descent optimization algorithms (Sebastian Ruder) and presented by Renato Rodrigues Silva" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Lecture 24 - Deep Learning - part II
### Material based on An overview of gradient descent optimization algorithms (Sebastian Ruder) and presented by Renato Rodrigues Silva
### Federal University of Goias.
### (updated: 2020-11-30)

---

class: middle

##Introduction

- Gradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks.

- Gradient descent is a way to minimize an objective function `\(J(\theta)\)` parameterized by a modelâ€™s parameters `\(\theta \in \mathbb{R}^d\)`  by updating the parameters in the opposite direction of the gradient of the objective function `\(\nabla_{\theta} J(\theta)\)` w.r.t. to the parameters.

- The learning rate `\(\eta\)` determines the size of the steps we take to reach a (local) minimum.


---
class: middle

##Introduction

- For a linear neuron with a squared error, it is a quadratic bowl function.

- Vertical cross-sections are parabolas.

- Horizontal cross-sections are ellipses.

- For multi-layer, non-linear nets the error surface is much more complicated.

- But locally, a piece of a quadratic bowl is usually a very good approximation.


---
class: middle

##Gradient descent variants

- There are three variants of gradient descent, which differ in how much data we use to compute the gradient of the objective function. 

- Depending on the amount of data, we make a trade-off between
the accuracy of the parameter update and the time it takes to perform an update.


---
class: middle

##Batch gradient descent

-   Vanilla gradient descent, aka batch gradient descent, computes the gradient of the cost function w.r.t. to the parameters `\(\theta\)` for the **entire training dataset**:

`$$\theta  = \theta - \eta \nabla_{\theta} J(\theta).$$`

-   As we need to calculate the **gradients** for the **whole dataset** to perform just one update, **batch gradient descent** can be **very slow** and is **intractable** for datasets that do not fit in memory. 

-   Batch gradient descent also does not allow us to update our model online, i.e. with new examples on-the-fly.

---
class: middle

## Stochastic gradient descent

-   Stochastic gradient descent (SGD) in contrast performs a parameter update for each training example `\(x^{(i)}\)` and label `\(y^{(i)}\)`.

`$$\theta  = \theta - \eta \nabla_{\theta} J(\theta; x^{(i)}; y^{(i)}).$$`
###Strength and drawback

- Batch gradient descent performs redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update.  

- While SGD does away with this redundancy by performing one update at a time.

- However, SGD can perform frequent updates with a high variance that causes the objective function to fluctuate heavily.

- Moreover, it has been shown that when we slowly decrease the learning rate, SGD
shows the same convergence behaviour as batch gradient descent, almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively. 


---
class: middle

##Mini-batch gradient descent

-   Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of `\(n\)` training examples:

`$$\theta  = \theta - \eta \nabla_{\theta} J(\theta; x^{(i;i+n)}; y^{(i;i+n)})$$`

####Strength and drawback

- Reduces the variance of the parameter updates, which can lead to more stable convergence


####Recomendations

- Common mini-batch sizes range between 50 and 256, but can vary for different applications


---
class: middle

##Challenges

- Going downhill reduces the error, but the direction of steepest descent does not point at the minimum unless the ellipse is a circle.

- The gradient is big in the direction in which we only want to travel a small distance.

-   The gradient is small in the direction in which we want to travel a large distance.

[Source:](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)

---
class: middle

##Challenges

-   A learning rate that is too small leads to  slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.

- Additionally, the same learning rate applies to all parameter updates. 

- If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features.

-   Another key challenge is minimizing highly non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima and/or saddle points.

- These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions.


---
class: middle

##Challenges

&lt;img src="Lecture_24_files/figure-html/Fig1.png" width="120%" align="center" /&gt;




---
class: middle

##Momentum - Intuition


-   Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations.

-   Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity, if there is air resistance i.e. `\(\gamma &lt; 1.\)`) 

-   Momentum	damps	oscillations	in	directions	of	high	curvature	by	combining	gradients	with	opposite	signs.



---
class: middle

##Momentum

-  The	effect of the	gradient is	to increment	the	previous	velocity.	The	
velocity also decays by `\(\alpha\)`	which	is slightly	less	then	1.


`$$\nu_t = \gamma \nu_{t-1} + \eta \nabla_{\theta} J(\theta).$$`
- The	weight change	is equal to	the	current	velocity.	


`$$\Delta \theta_t = \theta_t - \theta_{t-1} = \nu_t$$`

- The	weight	change	can	be	expressed	in	terms	of	the	previous	weight	change	and	the	current	gradient., i.e,

`\begin{align}
\Delta \theta_t =&amp; \nu_t \\
=&amp; \gamma \nu_{t-1} + \eta \nabla_{\theta} J(\theta) \\
=&amp; \gamma \Delta \theta_{t-1} + \eta \nabla_{\theta} J(\theta).
\end{align}`



---
class: middle

##Momentum - Summary

.pull-left[

###Mathematical Expression

`\begin{align}
\nu_t =&amp; \gamma \nu_{t-1} + \eta \nabla_{\theta} J(\theta) \\
\theta_t =&amp; \theta_{t-1} - \nu_t.
\end{align}`

]

.pull-right[




&lt;img src="Lecture_24_files/figure-html/Fig2.jpeg" width="140%" align="center" /&gt;



]

-   However, a ball that rolls down a hill, blindly following the slope, is highly unsatisfactory.
-   We would like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.

---
class: middle

##Nesterov accelerated gradient

- Nesterov accelerated gradient (NAG) is a way to give our momentum term this kind of foresight.

- We know that we will use our momentum term `\(\gamma \nu_{t-1}\)` to move the parameters `\(\theta.\)`

- Computing `\(\theta - \gamma \nu_{t-1}\)` thus gives us an approximation of the next position of the parameters (the gradient is missing for the full update), a rough idea where our parameters are going to be. 

- We can now effectively look ahead by calculating the gradient not w.r.t. to our current parameter `\(\theta\)` but w.r.t. the approximate future
position of our parameters:

- It dampens oscillations in	directions of high	curvature by	combining
gradients with opposite signs.


[Source:](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)


---
class: middle

##Nesterov accelerated gradient


.pull-left[

####Mathematical Expression

`\begin{align}
\nu_t =&amp; \gamma \nu_{t-1} + \eta \nabla_{\theta} J(\theta-\gamma \nu_{t-1}) \\
\theta_t =&amp; \theta_{t-1} - \nu_t.
\end{align}`


]


.pull-right[

&lt;img src="Lecture_24_files/figure-html/Fig3.png" width="100%" align="center" /&gt;



]

-   While Momentum first computes the current gradient (small blue vector in Figure 3) and then takes a big jump in the direction of the updated accumulated gradient (big blue vector).

-   NAG first makes a big jump in the direction of the previous accumulated gradient (brown vector), measures the gradient and then makes a correction
(green vector).


---
class: middle

##Rprop:


- The magnitude of the gradient can be very different for 
different weights and can change during learning.

- It makes hard to choose a single global learning rate.

- For full batch learning, we can deal with this variation by only
using the sign of the gradient.

- The weight updates are all of the same magnitude.

- This escapes from saddle points and plateaus  with tiny gradients quickly.

- Rprop combines the idea of only using the sign of the gradient with the idea of adapting the step size individually for each weight


---
class: middle
##Rprop

- Full batch only 

- Motivation: Gradient variance is big, hard to choose global learning rate

- Use gradient sign and depend on local learning rates

-   To adjust the step size for some weight, the following algorithm is used:

a.    Check last two signs, if they are the same, multiply by 1.2 otherwise by 0.5

b.    Now we can apply the weight update. `\(\theta_t = \theta_{t-1} -\mbox{sign}(\nabla J(\theta)) s\)`, where `\(s\)` is the step size.

[Source 1:](https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a)

[Source: 2](https://courses.cs.ut.ee/MTAT.03.277/2014_fall/uploads/Main/deep-learning-lecture-5-Optimization-how-to-make-the-learning-go-faster-hans%20-peeter-tulmin.pdf)

---
class: middle

##Rprop - Challenge

- Rprop doesnâ€™t really work when we have very large datasets and need to perform mini-batch weights updates. 

-  The reason it doesnâ€™t work is that it violates the central idea behind stochastic gradient descent, which is when we have small enough learning rate, it averages the gradients over successive mini-batches.

- Consider the weight, that gets the gradient 0.1 on nine mini-batches, and the gradient of -0.9 on tenths mini-batch. What weâ€™d like is to those gradients to roughly cancel each other out, so that the stay approximately the same.

-  With Rprop, we increment the weight 9 times and decrement only once, so the weight grows much larger.


---
class: middle

##RMSprop


- The central idea of RMSprop is keep the moving average of the squared gradients for each weight. 

- And then we divide the gradient by square root the mean square. The equation as follows

`\begin{align}
E[g^2]_t =&amp; 0.9 E[g^2]_{t-1} + 0.1 g_t^2 \\
\theta_{t+1} =&amp; \theta_t - \frac{\eta}{\sqrt{E[g^2_t] + \epsilon}} g_t
\end{align}`
where `\(E[g^2]\)` is the exponential decaying average of squared gradients.

- Hinton suggests `\(\gamma\)` to be set to 0.9. [Other Sources:](https://d2l.ai/chapter_optimization/rmsprop.html)


---
class: middle

##Adagrad

- Adagrad is an algorithm for gradient-based optimization that does just this: It adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters.

- For this reason, it is well-suited for dealing with sparse data (NLP and image recognition problem).

- Previously, we performed an update for all parameters `\(\theta\)` at once as every parameter `\(\theta_i\)` used the same learning rate `\(\eta.\)`

- Adagrad uses a different learning rate for every parameter `\(\theta_i\)` at every time step `\(t,\)`

---
class: middle

##Adagrad

- For brevity we set `\(g_{t,i}\)` to be the gradient of the objective function w.r.t to the parameter `\(\theta_i\)` at time step `\(t:\)`

`$$g_{t,i} = \nabla_{\theta_i} J(\theta_{t,i}).$$`

- The SGD update for every parameter `\(\theta_i\)` at each time step `\(t\)` then becomes:

`$$\theta_{t+1,i} = \theta_{t,i} - \eta g_{t,i}.$$`
- In its update rule, Adagrad modifies the general learning rate `\(\eta\)` at each time step `\(t\)` for every parameter `\(\theta_i\)` based on the past gradients that have been computed for `\(\theta_i:\)`


`$$\theta_{t+1,i} = \theta_{t,i} - \frac{\eta}{\sqrt{G_{t,{ii}}+\epsilon}} g_{t,i},$$`
where `\(G_t \in \mathbb{R}^{d \times d}\)` here is a diagonal matrix where each diagonal element `\(i,i\)` is the sum of the squares of the gradients w.r.t `\(\theta_i\)` up to time step `\(t,\)` while `\(\epsilon\)` is a smoothing term that avoids division by zero. 


---
class: middle

##Adam

- Combine the advantages of AdaGrad (works well with sparse gradients) and RMSProp (works well in non-stationary settings (e.g noisy)).

- Adam also keeps an exponentially decaying average of past gradients `\(m_t\)` similar to momentum:

`\begin{align}
m_t =&amp; \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
\nu_t =&amp; \beta_2 \nu_{t-1} + (1 - \beta_2)g_t^2,
\end{align}`
where `\(m_t\)` and `\(\nu_t\)` are estimates of the first moment (mean) and the second moments (the uncentered variance) of the gradients, respectively.

- The authors of Adam observe that `\(m_t\)` and `\(\nu_t\)` are biased towards zero, which can be corrected via

`$$\hat{m}_t = \frac{m_t}{1 - \beta^{t}_1}; \phantom{111} \hat{\nu}_t = \frac{\nu_t}{1 - \beta^{t}_2}.$$`
- The paramaters update can be done as follows: `\(\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{\nu}_t + \epsilon}}\hat{m}_t.\)`
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
